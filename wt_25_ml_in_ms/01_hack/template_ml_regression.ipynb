{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression template\n",
    "\n",
    "This follows https://auto.gluon.ai/stable/tutorials/tabular/tabular-quick-start.html\n",
    "\n",
    "Work through the notebook cells and change to make it work for your project\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chris/courses/wt-25-ml-in-ms/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# !pip install pandas numpy autogluon ipywidgets git+https://github.com/Ramprasad-Group/psmiles.git\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from psmiles import PolymerSmiles as PS\n",
    "from sklearn.model_selection import train_test_split\n",
    "from autogluon.tabular import TabularPredictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data and fingerprint\n",
    "\n",
    "- Create another notebook \"data.ipynb\". Synthesize, modify, manipulate, and save your data as pandas dataframe in this notebook. Finally, save your data using `pd.to_csv('data.csv)',\n",
    "- Replace the following code and load your own data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_init = pd.read_json(\n",
    "    \"https://raw.githubusercontent.com/kuennethgroup/materials_datasets/refs/heads/main/polymer_tendency_to_crystalize/polymers_tend_to_crystalize.json\"\n",
    ")[[\"smiles\", \"value\"]]\n",
    "\n",
    "# Compute the fingerprints using the PSMILES package\n",
    "fps = np.vstack(df_init.smiles.apply(lambda x: PS(x).fingerprint()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale the fingerprints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler_fps = MinMaxScaler()\n",
    "fps_scaled = scaler_fps.fit_transform(fps)\n",
    "fps_scaled = pd.DataFrame(fps_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare final data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>11</th>\n",
       "      <th>...</th>\n",
       "      <th>2035</th>\n",
       "      <th>2036</th>\n",
       "      <th>2038</th>\n",
       "      <th>2039</th>\n",
       "      <th>2041</th>\n",
       "      <th>2043</th>\n",
       "      <th>2044</th>\n",
       "      <th>2045</th>\n",
       "      <th>2047</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.483077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.449331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.343636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.201459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.217977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>427</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.321342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.257904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>429</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.293068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.218991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.075193</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>432 rows × 1369 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0         1    2    4    5    6    7    8    9   11  ...  2035  2036  \\\n",
       "0    0.0  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "1    0.0  0.166667  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "2    1.0  0.166667  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "3    0.0  0.166667  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "4    0.0  0.333333  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "..   ...       ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   ...   ...   \n",
       "427  0.0  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "428  0.0  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "429  0.0  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "430  0.0  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "431  0.0  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "\n",
       "     2038  2039  2041  2043  2044  2045  2047     value  \n",
       "0     0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.483077  \n",
       "1     0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.449331  \n",
       "2     0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.343636  \n",
       "3     0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.201459  \n",
       "4     0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.217977  \n",
       "..    ...   ...   ...   ...   ...   ...   ...       ...  \n",
       "427   0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.321342  \n",
       "428   0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.257904  \n",
       "429   0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.293068  \n",
       "430   0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.218991  \n",
       "431   0.0   0.0   0.0   0.0   0.0   0.0   0.5  0.075193  \n",
       "\n",
       "[432 rows x 1369 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concat fingerprints\n",
    "df = pd.concat((fps_scaled, df_init), axis=1)\n",
    "\n",
    "# drop smiles column because it should not be used for training\n",
    "df = df.drop(columns=\"smiles\")\n",
    "\n",
    "# Make sure they're all float\n",
    "df = df.astype(np.float32)\n",
    "\n",
    "\n",
    "# Remove columns that are zero, if any\n",
    "df = df.loc[:, (df != 0).any(axis=0)]\n",
    "\n",
    "# Normalize the tendency to crystalize\n",
    "# df['value'] = df['value'] / 100\n",
    "\n",
    "scaler_value = MinMaxScaler()\n",
    "df[\"value\"] = scaler_value.fit_transform(df[[\"value\"]])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split in train and test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>11</th>\n",
       "      <th>...</th>\n",
       "      <th>2035</th>\n",
       "      <th>2036</th>\n",
       "      <th>2038</th>\n",
       "      <th>2039</th>\n",
       "      <th>2041</th>\n",
       "      <th>2043</th>\n",
       "      <th>2044</th>\n",
       "      <th>2045</th>\n",
       "      <th>2047</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.582590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.286178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.217673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.809282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.300365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.212606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.088873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.320488</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>345 rows × 1369 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0         1    2    4    5    6    7    8    9   11  ...  2035  2036  \\\n",
       "132  0.0  0.166667  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "231  0.0  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "31   0.0  0.166667  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "84   0.0  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "296  0.0  0.166667  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "..   ...       ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   ...   ...   \n",
       "71   0.0  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "106  0.0  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.5   \n",
       "270  0.0  0.166667  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "348  0.0  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   \n",
       "102  0.0  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.5   \n",
       "\n",
       "     2038  2039  2041  2043  2044  2045  2047     value  \n",
       "132   0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.582590  \n",
       "231   0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.286178  \n",
       "31    0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.217673  \n",
       "84    0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.809282  \n",
       "296   0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.002432  \n",
       "..    ...   ...   ...   ...   ...   ...   ...       ...  \n",
       "71    0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.300365  \n",
       "106   0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.212606  \n",
       "270   0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.088873  \n",
       "348   0.0   0.0   1.0   0.0   0.0   0.0   0.0  0.125253  \n",
       "102   0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.320488  \n",
       "\n",
       "[345 rows x 1369 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>11</th>\n",
       "      <th>...</th>\n",
       "      <th>2035</th>\n",
       "      <th>2036</th>\n",
       "      <th>2038</th>\n",
       "      <th>2039</th>\n",
       "      <th>2041</th>\n",
       "      <th>2043</th>\n",
       "      <th>2044</th>\n",
       "      <th>2045</th>\n",
       "      <th>2047</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>424</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.083604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.733685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.536279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.168221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.385083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.635488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.894406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.126752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.373835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.231557</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>87 rows × 1369 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0         1    2    4    5    6    7    8     9   11  ...  2035  2036  \\\n",
       "424  0.0  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.00  0.0  ...   0.0   0.0   \n",
       "75   0.0  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.25  0.0  ...   0.0   0.0   \n",
       "180  0.0  0.000000  0.0  0.0  0.0  0.0  0.0  0.5  0.00  0.0  ...   0.0   0.0   \n",
       "30   0.0  0.166667  0.0  0.0  0.0  0.0  0.0  0.0  0.00  0.0  ...   0.0   0.0   \n",
       "392  0.0  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.00  0.5  ...   0.0   0.0   \n",
       "..   ...       ...  ...  ...  ...  ...  ...  ...   ...  ...  ...   ...   ...   \n",
       "57   0.0  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.00  0.0  ...   0.0   0.0   \n",
       "124  0.0  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.00  0.0  ...   0.0   0.0   \n",
       "24   0.0  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.00  0.0  ...   0.0   0.0   \n",
       "17   0.0  0.166667  0.0  0.0  0.0  0.0  0.0  0.0  0.00  0.0  ...   0.0   0.0   \n",
       "66   0.0  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.00  0.0  ...   0.0   0.0   \n",
       "\n",
       "     2038  2039  2041  2043  2044  2045  2047     value  \n",
       "424   0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.083604  \n",
       "75    0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.733685  \n",
       "180   0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.536279  \n",
       "30    0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.168221  \n",
       "392   0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.385083  \n",
       "..    ...   ...   ...   ...   ...   ...   ...       ...  \n",
       "57    0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.635488  \n",
       "124   0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.894406  \n",
       "24    0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.126752  \n",
       "17    0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.373835  \n",
       "66    0.0   0.0   0.0   0.0   0.0   0.0   0.0  0.231557  \n",
       "\n",
       "[87 rows x 1369 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_train, df_test = train_test_split(df, test_size=0.20, random_state=42)\n",
    "\n",
    "display(df_train)\n",
    "display(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train you AutoGluon ML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20241218_092407\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.2\n",
      "Python Version:     3.10.12\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #135-Ubuntu SMP Fri Sep 27 13:53:58 UTC 2024\n",
      "CPU Count:          192\n",
      "Memory Avail:       986.94 GB / 1007.45 GB (98.0%)\n",
      "Disk Space Avail:   1657.45 GB / 7096.34 GB (23.4%)\n",
      "===================================================\n",
      "Presets specified: ['best_quality']\n",
      "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
      "\tRunning DyStack for up to 15s of the 60s of remaining time (25%).\n",
      "2024-12-18 10:24:07,444\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "\tRunning DyStack sub-fit in a ray process to avoid memory leakage. Enabling ray logging (enable_ray_logging=True). Specify `ds_args={'enable_ray_logging': False}` if you experience logging issues.\n",
      "2024-12-18 10:24:09,370\tINFO worker.py:1810 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\t\tContext path: \"/home/chris/courses/wt-25-ml-in-ms/wt_25_ml_in_ms/01_hack/AutogluonModels/ag-20241218_092407/ds_sub_fit/sub_fit_ho\"\n",
      "\u001b[36m(_dystack pid=978279)\u001b[0m Running DyStack sub-fit ...\n",
      "\u001b[36m(_dystack pid=978279)\u001b[0m Beginning AutoGluon training ... Time limit = 12s\n",
      "\u001b[36m(_dystack pid=978279)\u001b[0m AutoGluon will save models to \"/home/chris/courses/wt-25-ml-in-ms/wt_25_ml_in_ms/01_hack/AutogluonModels/ag-20241218_092407/ds_sub_fit/sub_fit_ho\"\n",
      "\u001b[36m(_dystack pid=978279)\u001b[0m Train Data Rows:    306\n",
      "\u001b[36m(_dystack pid=978279)\u001b[0m Train Data Columns: 1368\n",
      "\u001b[36m(_dystack pid=978279)\u001b[0m Label Column:       value\n",
      "\u001b[36m(_dystack pid=978279)\u001b[0m Problem Type:       regression\n",
      "\u001b[36m(_dystack pid=978279)\u001b[0m Preprocessing data ...\n",
      "\u001b[36m(_dystack pid=978279)\u001b[0m Using Feature Generators to preprocess the data ...\n",
      "\u001b[36m(_dystack pid=978279)\u001b[0m Fitting AutoMLPipelineFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=978279)\u001b[0m \tAvailable Memory:                    1001710.81 MB\n",
      "\u001b[36m(_dystack pid=978279)\u001b[0m \tTrain Data (Original)  Memory Usage: 1.60 MB (0.0% of available memory)\n",
      "\u001b[36m(_dystack pid=978279)\u001b[0m \tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\u001b[36m(_dystack pid=978279)\u001b[0m \tStage 1 Generators:\n",
      "\u001b[36m(_dystack pid=978279)\u001b[0m \t\tFitting AsTypeFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=978279)\u001b[0m \t\t\tNote: Converting 815 features to boolean dtype as they only contain 2 unique values.\n",
      "\u001b[36m(_dystack pid=978279)\u001b[0m \tStage 2 Generators:\n",
      "\u001b[36m(_dystack pid=978279)\u001b[0m \t\tFitting FillNaFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=978279)\u001b[0m \tStage 3 Generators:\n",
      "\u001b[36m(_dystack pid=978279)\u001b[0m \t\tFitting IdentityFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=978279)\u001b[0m \tStage 4 Generators:\n",
      "\u001b[36m(_dystack pid=978279)\u001b[0m \t\tFitting DropUniqueFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=978279)\u001b[0m \tStage 5 Generators:\n",
      "\u001b[36m(_dystack pid=978279)\u001b[0m \t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=978279)\u001b[0m \tUseless Original Features (Count: 144): ['0', '6', '12', '20', '35', '61', '88', '119', '136', '155', '165', '179', '181', '207', '208', '257', '262', '267', '320', '324', '332', '365', '370', '384', '395', '412', '414', '421', '470', '496', '500', '517', '529', '530', '531', '550', '558', '610', '624', '631', '638', '644', '652', '657', '712', '729', '731', '737', '747', '752', '764', '765', '768', '771', '778', '786', '800', '808', '813', '815', '821', '839', '849', '874', '876', '885', '927', '943', '962', '970', '990', '993', '1001', '1005', '1041', '1097', '1124', '1130', '1131', '1148', '1169', '1186', '1190', '1193', '1207', '1228', '1247', '1255', '1264', '1268', '1271', '1284', '1295', '1298', '1307', '1330', '1363', '1373', '1405', '1410', '1418', '1438', '1448', '1455', '1468', '1478', '1483', '1512', '1521', '1528', '1537', '1539', '1540', '1545', '1561', '1597', '1613', '1630', '1644', '1653', '1698', '1704', '1709', '1724', '1725', '1732', '1733', '1741', '1744', '1751', '1756', '1761', '1801', '1861', '1880', '1892', '1912', '1961', '1972', '1997', '2008', '2014', '2038', '2044']\n",
      "\u001b[36m(_dystack pid=978279)\u001b[0m \t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\u001b[36m(_dystack pid=978279)\u001b[0m \t\tThis is typically a feature which has the same value for all rows.\n",
      "\u001b[36m(_dystack pid=978279)\u001b[0m \t\tThese features do not need to be present at inference time.\n",
      "\u001b[36m(_dystack pid=978279)\u001b[0m \tUnused Original Features (Count: 383): ['30', '47', '67', '75', '146', '157', '163', '185', '199', '201', '204', '234', '246', '254', '255', '260', '265', '271', '285', '301', '304', '305', '306', '307', '315', '317', '318', '341', '347', '353', '364', '372', '377', '381', '387', '406', '411', '428', '445', '451', '472', '485', '490', '498', '499', '502', '509', '512', '516', '525', '527', '537', '538', '544', '549', '561', '563', '567', '568', '582', '585', '599', '600', '603', '629', '633', '643', '648', '649', '653', '654', '655', '658', '659', '663', '666', '675', '681', '701', '707', '719', '723', '726', '741', '750', '761', '767', '772', '774', '776', '779', '796', '797', '798', '802', '810', '811', '812', '814', '822', '825', '828', '838', '845', '853', '860', '863', '871', '877', '880', '895', '897', '904', '906', '914', '916', '925', '936', '941', '946', '954', '960', '966', '968', '979', '984', '986', '1004', '1020', '1027', '1032', '1042', '1048', '1049', '1050', '1051', '1056', '1060', '1061', '1062', '1067', '1068', '1069', '1076', '1080', '1082', '1083', '1084', '1085', '1089', '1091', '1095', '1098', '1100', '1108', '1109', '1111', '1118', '1123', '1125', '1126', '1127', '1128', '1138', '1140', '1142', '1155', '1165', '1167', '1168', '1172', '1173', '1175', '1184', '1187', '1189', '1195', '1197', '1203', '1208', '1210', '1215', '1219', '1222', '1223', '1225', '1235', '1239', '1245', '1248', '1262', '1266', '1278', '1289', '1296', '1306', '1309', '1310', '1322', '1323', '1324', '1331', '1332', '1333', '1335', '1347', '1348', '1353', '1357', '1360', '1362', '1372', '1374', '1378', '1379', '1383', '1393', '1400', '1409', '1411', '1413', '1415', '1416', '1421', '1424', '1439', '1445', '1457', '1463', '1464', '1467', '1470', '1472', '1479', '1481', '1487', '1489', '1490', '1492', '1499', '1503', '1506', '1508', '1514', '1515', '1516', '1517', '1523', '1527', '1530', '1531', '1533', '1534', '1542', '1553', '1557', '1559', '1569', '1573', '1576', '1580', '1585', '1604', '1605', '1606', '1607', '1608', '1609', '1611', '1614', '1616', '1618', '1619', '1624', '1625', '1627', '1633', '1634', '1639', '1646', '1650', '1654', '1656', '1659', '1672', '1674', '1685', '1692', '1708', '1710', '1712', '1713', '1720', '1721', '1726', '1735', '1738', '1745', '1746', '1749', '1760', '1762', '1764', '1773', '1774', '1778', '1779', '1781', '1782', '1785', '1786', '1791', '1793', '1796', '1802', '1803', '1805', '1807', '1812', '1814', '1819', '1823', '1827', '1829', '1834', '1844', '1845', '1848', '1850', '1858', '1860', '1862', '1863', '1865', '1867', '1868', '1870', '1877', '1883', '1889', '1894', '1898', '1899', '1900', '1901', '1904', '1905', '1908', '1913', '1919', '1922', '1923', '1925', '1929', '1932', '1935', '1937', '1939', '1941', '1950', '1955', '1960', '1965', '1968', '1971', '1975', '1983', '1984', '1993', '2001', '2002', '2003', '2006', '2013', '2021', '2022', '2026', '2031', '2032', '2034', '2039', '2043', '2045']\n",
      "\u001b[36m(_dystack pid=978279)\u001b[0m \t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\u001b[36m(_dystack pid=978279)\u001b[0m \t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\u001b[36m(_dystack pid=978279)\u001b[0m \t\tThese features do not need to be present at inference time.\n",
      "\u001b[36m(_dystack pid=978279)\u001b[0m \t\t('float', []) : 383 | ['30', '47', '67', '75', '146', ...]\n",
      "\u001b[36m(_dystack pid=978279)\u001b[0m \tTypes of features in original data (raw dtype, special dtypes):\n",
      "\u001b[36m(_dystack pid=978279)\u001b[0m \t\t('float', []) : 841 | ['1', '2', '4', '5', '7', ...]\n",
      "\u001b[36m(_dystack pid=978279)\u001b[0m \tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\u001b[36m(_dystack pid=978279)\u001b[0m \t\t('float', [])     : 388 | ['1', '8', '9', '11', '13', ...]\n",
      "\u001b[36m(_dystack pid=978279)\u001b[0m \t\t('int', ['bool']) : 453 | ['2', '4', '5', '7', '14', ...]\n",
      "\u001b[36m(_dystack pid=978279)\u001b[0m \t2.3s = Fit runtime\n",
      "\u001b[36m(_dystack pid=978279)\u001b[0m \t841 features in original data used to generate 841 features in processed data.\n",
      "\u001b[36m(_dystack pid=978279)\u001b[0m \tTrain Data (Processed) Memory Usage: 0.59 MB (0.0% of available memory)\n",
      "\u001b[36m(_dystack pid=978279)\u001b[0m Data preprocessing and feature engineering runtime = 2.34s ...\n",
      "\u001b[36m(_dystack pid=978279)\u001b[0m AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\u001b[36m(_dystack pid=978279)\u001b[0m \tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\u001b[36m(_dystack pid=978279)\u001b[0m \tTo change this, specify the eval_metric parameter of Predictor()\n",
      "\u001b[36m(_dystack pid=978279)\u001b[0m Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "\u001b[36m(_dystack pid=978279)\u001b[0m User-specified model hyperparameters to be fit:\n",
      "\u001b[36m(_dystack pid=978279)\u001b[0m {\n",
      "\u001b[36m(_dystack pid=978279)\u001b[0m \t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\u001b[36m(_dystack pid=978279)\u001b[0m \t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\u001b[36m(_dystack pid=978279)\u001b[0m \t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\u001b[36m(_dystack pid=978279)\u001b[0m \t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\u001b[36m(_dystack pid=978279)\u001b[0m \t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\u001b[36m(_dystack pid=978279)\u001b[0m \t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\u001b[36m(_dystack pid=978279)\u001b[0m \t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\u001b[36m(_dystack pid=978279)\u001b[0m \t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "\u001b[36m(_dystack pid=978279)\u001b[0m }\n",
      "\u001b[36m(_dystack pid=978279)\u001b[0m AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "\u001b[36m(_dystack pid=978279)\u001b[0m Fitting 108 L1 models, fit_strategy=\"sequential\" ...\n",
      "\u001b[36m(_dystack pid=978279)\u001b[0m Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 6.23s of the 9.34s of remaining time.\n",
      "\u001b[36m(_dystack pid=978279)\u001b[0m \t-0.2152\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=978279)\u001b[0m \t0.03s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=978279)\u001b[0m \t0.29s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=978279)\u001b[0m Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 5.82s of the 8.93s of remaining time.\n",
      "\u001b[36m(_dystack pid=978279)\u001b[0m \t-0.2107\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=978279)\u001b[0m \t0.03s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=978279)\u001b[0m \t0.25s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=978279)\u001b[0m Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 5.45s of the 8.56s of remaining time.\n",
      "\u001b[36m(_dystack pid=978279)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.02%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=999748)\u001b[0m [1000]\tvalid_set's rmse: 0.135959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=978279)\u001b[0m \t-0.1891\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=978279)\u001b[0m \t1.36s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=978279)\u001b[0m \t0.03s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=978279)\u001b[0m Fitting model: LightGBM_BAG_L1 ... Training model for up to 2.73s of the 5.84s of remaining time.\n",
      "\u001b[36m(_dystack pid=978279)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.02%)\n",
      "\u001b[36m(_dystack pid=978279)\u001b[0m \t-0.1937\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=978279)\u001b[0m \t0.97s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=978279)\u001b[0m \t0.02s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=978279)\u001b[0m Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 0.41s of the 3.52s of remaining time.\n",
      "\u001b[36m(_dystack pid=978279)\u001b[0m \t-0.1972\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=978279)\u001b[0m \t0.78s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=978279)\u001b[0m \t0.09s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=978279)\u001b[0m Fitting model: WeightedEnsemble_L2 ... Training model for up to 9.35s of the 2.37s of remaining time.\n",
      "\u001b[36m(_dystack pid=978279)\u001b[0m \tEnsemble Weights: {'LightGBMXT_BAG_L1': 0.688, 'RandomForestMSE_BAG_L1': 0.188, 'KNeighborsDist_BAG_L1': 0.125}\n",
      "\u001b[36m(_dystack pid=978279)\u001b[0m \t-0.1875\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=978279)\u001b[0m \t0.01s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=978279)\u001b[0m \t0.0s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=978279)\u001b[0m Fitting 106 L2 models, fit_strategy=\"sequential\" ...\n",
      "\u001b[36m(_dystack pid=978279)\u001b[0m Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 2.31s of the 2.30s of remaining time.\n",
      "\u001b[36m(_dystack pid=978279)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.02%)\n",
      "\u001b[36m(_dystack pid=978279)\u001b[0m \t-0.1938\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=978279)\u001b[0m \t0.68s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=978279)\u001b[0m \t0.03s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=978279)\u001b[0m Fitting model: LightGBM_BAG_L2 ... Training model for up to 0.31s of the 0.29s of remaining time.\n",
      "\u001b[36m(_dystack pid=978279)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.02%)\n",
      "\u001b[36m(_dystack pid=978279)\u001b[0m \t-0.2385\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=978279)\u001b[0m \t0.31s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=978279)\u001b[0m \t0.02s\t = Validation runtime\n",
      "\u001b[36m(_ray_fit pid=1004141)\u001b[0m \tRan out of time, early stopping on iteration 1. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=1004141)\u001b[0m \t[1]\tvalid_set's rmse: 0.247326\n",
      "\u001b[36m(_dystack pid=978279)\u001b[0m Fitting model: WeightedEnsemble_L3 ... Training model for up to 9.35s of the -1.42s of remaining time.\n",
      "\u001b[36m(_dystack pid=978279)\u001b[0m \tEnsemble Weights: {'LightGBMXT_BAG_L1': 0.688, 'RandomForestMSE_BAG_L1': 0.188, 'KNeighborsDist_BAG_L1': 0.125}\n",
      "\u001b[36m(_dystack pid=978279)\u001b[0m \t-0.1875\t = Validation score   (-root_mean_squared_error)\n",
      "\u001b[36m(_dystack pid=978279)\u001b[0m \t0.01s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=978279)\u001b[0m \t0.0s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=978279)\u001b[0m AutoGluon training complete, total runtime = 13.15s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 558.8 rows/s (39 batch size)\n",
      "\u001b[36m(_dystack pid=978279)\u001b[0m TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/home/chris/courses/wt-25-ml-in-ms/wt_25_ml_in_ms/01_hack/AutogluonModels/ag-20241218_092407/ds_sub_fit/sub_fit_ho\")\n",
      "\u001b[36m(_dystack pid=978279)\u001b[0m Deleting DyStack predictor artifacts (clean_up_fits=True) ...\n",
      "Leaderboard on holdout data (DyStack):\n",
      "                    model  score_holdout  score_val              eval_metric  pred_time_test  pred_time_val  fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0         LightGBM_BAG_L1      -0.143172  -0.193740  root_mean_squared_error        0.187271       0.024717  0.966233                 0.187271                0.024717           0.966233            1       True          4\n",
      "1       LightGBMXT_BAG_L1      -0.144551  -0.189068  root_mean_squared_error        0.309134       0.026144  1.364032                 0.309134                0.026144           1.364032            1       True          3\n",
      "2       LightGBMXT_BAG_L2      -0.145916  -0.193825  root_mean_squared_error        0.800467       0.397146  2.857213                 0.061215                0.028882           0.682517            2       True          7\n",
      "3     WeightedEnsemble_L3      -0.148491  -0.187483  root_mean_squared_error        0.740767       0.368553  2.179821                 0.001516                0.000289           0.005124            3       True          9\n",
      "4     WeightedEnsemble_L2      -0.148491  -0.187483  root_mean_squared_error        0.741430       0.368575  2.182292                 0.002179                0.000312           0.007596            2       True          6\n",
      "5  RandomForestMSE_BAG_L1      -0.165186  -0.197151  root_mean_squared_error        0.156633       0.091220  0.779828                 0.156633                0.091220           0.779828            1       True          5\n",
      "6   KNeighborsUnif_BAG_L1      -0.179982  -0.215233  root_mean_squared_error        0.229917       0.285592  0.030092                 0.229917                0.285592           0.030092            1       True          1\n",
      "7   KNeighborsDist_BAG_L1      -0.183919  -0.210714  root_mean_squared_error        0.273484       0.250900  0.030837                 0.273484                0.250900           0.030837            1       True          2\n",
      "8         LightGBM_BAG_L2      -0.194571  -0.238468  root_mean_squared_error        0.798013       0.392520  2.489449                 0.058762                0.024256           0.314752            2       True          8\n",
      "\t1\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n",
      "\t19s\t = DyStack   runtime |\t41s\t = Remaining runtime\n",
      "Starting main fit with num_stack_levels=1.\n",
      "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`\n",
      "Beginning AutoGluon training ... Time limit = 41s\n",
      "AutoGluon will save models to \"/home/chris/courses/wt-25-ml-in-ms/wt_25_ml_in_ms/01_hack/AutogluonModels/ag-20241218_092407\"\n",
      "Train Data Rows:    345\n",
      "Train Data Columns: 1368\n",
      "Label Column:       value\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    1007823.61 MB\n",
      "\tTrain Data (Original)  Memory Usage: 1.80 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 843 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 100): ['6', '35', '88', '119', '136', '155', '179', '181', '207', '332', '365', '370', '384', '395', '412', '414', '421', '470', '496', '500', '517', '529', '531', '550', '610', '624', '631', '712', '731', '737', '747', '752', '765', '768', '771', '778', '786', '800', '808', '813', '815', '821', '849', '874', '876', '885', '927', '943', '962', '970', '990', '1001', '1005', '1041', '1097', '1124', '1130', '1131', '1148', '1169', '1186', '1190', '1207', '1247', '1264', '1268', '1284', '1295', '1298', '1307', '1373', '1405', '1410', '1418', '1438', '1448', '1468', '1478', '1483', '1528', '1537', '1561', '1613', '1644', '1698', '1704', '1709', '1724', '1732', '1744', '1756', '1761', '1801', '1861', '1880', '1892', '1972', '2014', '2038', '2044']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 387): ['47', '61', '67', '75', '146', '157', '163', '185', '199', '201', '204', '208', '234', '246', '254', '255', '257', '260', '262', '265', '271', '285', '304', '305', '306', '315', '317', '318', '320', '324', '341', '353', '364', '372', '377', '381', '406', '428', '445', '451', '472', '485', '490', '498', '499', '502', '509', '512', '516', '525', '527', '537', '538', '549', '558', '561', '563', '567', '568', '582', '585', '599', '600', '603', '629', '633', '638', '643', '644', '648', '649', '653', '654', '655', '657', '658', '659', '663', '666', '675', '681', '701', '707', '723', '726', '729', '741', '750', '764', '767', '772', '774', '776', '779', '796', '797', '798', '802', '810', '811', '812', '822', '825', '828', '838', '839', '845', '853', '860', '863', '871', '877', '880', '895', '897', '904', '906', '914', '916', '925', '936', '941', '946', '960', '966', '968', '979', '984', '986', '993', '1004', '1020', '1027', '1032', '1042', '1048', '1049', '1050', '1051', '1060', '1061', '1062', '1067', '1068', '1069', '1080', '1082', '1084', '1085', '1089', '1091', '1095', '1098', '1100', '1108', '1109', '1111', '1118', '1123', '1125', '1126', '1127', '1140', '1142', '1155', '1165', '1167', '1168', '1172', '1173', '1175', '1184', '1187', '1189', '1193', '1195', '1197', '1203', '1208', '1210', '1215', '1219', '1222', '1223', '1225', '1228', '1235', '1239', '1245', '1248', '1266', '1271', '1278', '1289', '1296', '1306', '1310', '1322', '1324', '1330', '1331', '1332', '1333', '1335', '1347', '1348', '1353', '1360', '1362', '1363', '1372', '1374', '1378', '1379', '1383', '1393', '1400', '1409', '1411', '1413', '1416', '1421', '1424', '1439', '1445', '1455', '1457', '1463', '1464', '1467', '1470', '1472', '1479', '1481', '1487', '1489', '1490', '1492', '1503', '1506', '1508', '1512', '1514', '1515', '1516', '1517', '1521', '1523', '1527', '1530', '1531', '1534', '1540', '1542', '1545', '1553', '1557', '1559', '1569', '1573', '1576', '1580', '1585', '1597', '1604', '1605', '1606', '1607', '1608', '1609', '1611', '1614', '1618', '1619', '1624', '1625', '1627', '1630', '1633', '1634', '1639', '1646', '1650', '1653', '1654', '1656', '1659', '1672', '1674', '1685', '1692', '1708', '1710', '1712', '1713', '1720', '1721', '1725', '1726', '1733', '1735', '1738', '1741', '1745', '1746', '1751', '1760', '1762', '1764', '1773', '1778', '1779', '1781', '1782', '1786', '1791', '1793', '1796', '1803', '1805', '1807', '1812', '1814', '1819', '1823', '1827', '1829', '1834', '1844', '1845', '1848', '1850', '1858', '1860', '1862', '1863', '1865', '1867', '1868', '1870', '1877', '1883', '1894', '1898', '1899', '1900', '1901', '1904', '1905', '1908', '1912', '1913', '1919', '1922', '1923', '1925', '1929', '1932', '1935', '1937', '1939', '1941', '1950', '1955', '1960', '1961', '1965', '1968', '1971', '1975', '1983', '1993', '1997', '2001', '2003', '2006', '2008', '2013', '2021', '2022', '2026', '2031', '2032', '2034', '2039', '2043', '2045']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('float', []) : 387 | ['47', '61', '67', '75', '146', ...]\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 881 | ['0', '1', '2', '4', '5', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 408 | ['1', '2', '8', '9', '11', ...]\n",
      "\t\t('int', ['bool']) : 473 | ['0', '4', '5', '7', '12', ...]\n",
      "\t2.3s = Fit runtime\n",
      "\t881 features in original data used to generate 881 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.69 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 2.37s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 108 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 25.73s of the 38.60s of remaining time.\n",
      "\t-0.2062\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.26s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 25.34s of the 38.20s of remaining time.\n",
      "\t-0.2038\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.11s\t = Validation runtime\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 25.12s of the 37.99s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.02%)\n",
      "\t-0.1838\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.95s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 22.83s of the 35.69s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.02%)\n",
      "\t-0.1888\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.91s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 20.56s of the 33.43s of remaining time.\n",
      "\t-0.1929\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.64s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 19.68s of the 32.55s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.03%)\n",
      "\t-0.1848\t = Validation score   (-root_mean_squared_error)\n",
      "\t6.27s\t = Training   runtime\n",
      "\t0.26s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE_BAG_L1 ... Training model for up to 12.01s of the 24.87s of remaining time.\n",
      "\t-0.1947\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.67s\t = Training   runtime\n",
      "\t0.11s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 11.10s of the 23.97s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "\t-0.198\t = Validation score   (-root_mean_squared_error)\n",
      "\t4.05s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 5.67s of the 18.54s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.03%)\n",
      "\t-0.1883\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.42s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 2.91s of the 15.77s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
      "\tTime limit exceeded... Skipping NeuralNetTorch_BAG_L1.\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 38.60s of the 9.93s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 0.467, 'CatBoost_BAG_L1': 0.267, 'XGBoost_BAG_L1': 0.2, 'NeuralNetFastAI_BAG_L1': 0.067}\n",
      "\t-0.1791\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting 106 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 9.87s of the 9.84s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.02%)\n",
      "\t-0.1825\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.8s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 7.75s of the 7.72s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.02%)\n",
      "\t-0.1841\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.02s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L2 ... Training model for up to 5.35s of the 5.33s of remaining time.\n",
      "2024-12-18 10:25:02,439\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2024-12-18 10:25:02,489\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2024-12-18 10:25:02,540\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2024-12-18 10:25:02,559\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2024-12-18 10:25:02,628\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2024-12-18 10:25:02,641\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2024-12-18 10:25:02,662\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\t-0.1854\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.64s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L2 ... Training model for up to 4.49s of the 4.46s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.03%)\n",
      "\t-0.1789\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.91s\t = Training   runtime\n",
      "\t0.24s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE_BAG_L2 ... Training model for up to 1.15s of the 1.13s of remaining time.\n",
      "\t-0.1816\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.53s\t = Training   runtime\n",
      "\t0.11s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 38.60s of the 0.29s of remaining time.\n",
      "\tEnsemble Weights: {'CatBoost_BAG_L2': 0.579, 'ExtraTreesMSE_BAG_L2': 0.316, 'NeuralNetFastAI_BAG_L1': 0.053, 'XGBoost_BAG_L1': 0.053}\n",
      "\t-0.1782\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 40.75s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 61.1 rows/s (44 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/home/chris/courses/wt-25-ml-in-ms/wt_25_ml_in_ms/01_hack/AutogluonModels/ag-20241218_092407\")\n"
     ]
    }
   ],
   "source": [
    "predictor = TabularPredictor(\n",
    "    label=\"value\",\n",
    "    problem_type=\"regression\",\n",
    ").fit(df_train, time_limit=60, presets=\"best_quality\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use matplotlib for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2: 0.4705328345298767\n",
      "RMSE: 0.18199115991592407\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score, root_mean_squared_error\n",
    "\n",
    "y_pred = predictor.predict(df_test.drop(columns=[\"value\"]))\n",
    "\n",
    "r2 = r2_score(df_test[\"value\"], y_pred)\n",
    "rmse = root_mean_squared_error(df_test[\"value\"], y_pred)\n",
    "\n",
    "print(f\"R2: {r2}\")\n",
    "print(f\"RMSE: {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'pred')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "y_pred = predictor.predict(df_test.drop(columns=[\"value\"]))\n",
    "y_pred\n",
    "\n",
    "ax.plot(y_pred, df_test[\"value\"], \"o\")\n",
    "ax.plot([0, 1], [0, 1], \"k--\")\n",
    "ax.text(0.1, 0.9, f\"R2 = {r2:.3f}\", transform=ax.transAxes)\n",
    "ax.text(0.1, 0.85, f\"RMSE = {rmse:.3f}\", transform=ax.transAxes)\n",
    "ax.set_title(\"Testdata set\")\n",
    "ax.set_ylabel(\"truth\")\n",
    "ax.set_xlabel(\"pred\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
