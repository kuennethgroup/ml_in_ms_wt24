{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Just a neural network in Pytorch and tensorflow\n",
        "\n",
        "<!--<badge>--><a href=\"https://colab.research.google.com/github/kuennethgroup/ml_in_ms_wt24/blob/main/wt_25_ml_in_ms/07_ex/my_first_nn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a><!--</badge>-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MyNeuralNetwork(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "from torch import nn\n",
        "\n",
        "\n",
        "# Define model\n",
        "class MyNeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(28 * 28, 512),  # Input, Output\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 10),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "\n",
        "device = \"cpu\"\n",
        "model = MyNeuralNetwork().to(device)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"my_neural_network_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_6 (Flatten)         multiple                  0         \n",
            "                                                                 \n",
            " dense_18 (Dense)            multiple                  262656    \n",
            "                                                                 \n",
            " dense_19 (Dense)            multiple                  5632      \n",
            "                                                                 \n",
            " dense_20 (Dense)            multiple                  7850      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 276,138\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_6 (Flatten)         multiple                  0         \n",
            "                                                                 \n",
            " dense_18 (Dense)            multiple                  262656    \n",
            "                                                                 \n",
            " dense_19 (Dense)            multiple                  5632      \n",
            "                                                                 \n",
            " dense_20 (Dense)            multiple                  7850      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 276,138\n",
            "Trainable params: 276,138\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Define model\n",
        "class MyNeuralNetwork(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = tf.keras.layers.Flatten()\n",
        "\n",
        "        self.l1 = tf.keras.layers.Dense(512, activation=\"relu\")\n",
        "        self.l2 = tf.keras.layers.Dense(512, activation=\"relu\")\n",
        "        self.l3 = tf.keras.layers.Dense(10)\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.l1(self.l2(self.l3(x)))\n",
        "        return logits\n",
        "\n",
        "\n",
        "# Create model and print summary\n",
        "model = MyNeuralNetwork()\n",
        "\n",
        "inp = tf.keras.Input(shape=(28 * 28), batch_size=10)\n",
        "model(inp)\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exercise\n",
        "\n",
        "1. Use copilot or any LLM to help you understand each line of code above.\n",
        "2. Enhance both the PyTorch and TensorFlow models by adding a 4th dense layer with 200 neurons. Execute the code and observe the changes in the network architecture.\n",
        "3. Introduce dropout layers after each dense layer, except for the final layer.\n",
        "4. Replace all activation functions with PReLU."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}