{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# AutoML\n",
        "\n",
        "<!--<badge>--><a href=\"https://colab.research.google.com/github/kuennethgroup/ml_in_ms_wt24/blob/main/wt_25_ml_in_ms/08_ex/autoML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a><!--</badge>-->\n",
        "\n",
        "1. Prepare data\n",
        "1. We train a AutoML model using this dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>smiles</th>\n",
              "      <th>property</th>\n",
              "      <th>value</th>\n",
              "      <th>fingerprint</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[*]C[*]</td>\n",
              "      <td>Xc</td>\n",
              "      <td>47.80</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[*]CC([*])C</td>\n",
              "      <td>Xc</td>\n",
              "      <td>44.47</td>\n",
              "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[*]CC([*])CC</td>\n",
              "      <td>Xc</td>\n",
              "      <td>34.04</td>\n",
              "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[*]CC([*])CCC</td>\n",
              "      <td>Xc</td>\n",
              "      <td>20.01</td>\n",
              "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[*]CC([*])CC(C)C</td>\n",
              "      <td>Xc</td>\n",
              "      <td>21.64</td>\n",
              "      <td>[0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>427</th>\n",
              "      <td>[*]C([*])(F)F</td>\n",
              "      <td>Xc</td>\n",
              "      <td>31.84</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>428</th>\n",
              "      <td>[*]C/C=C\\C[*]</td>\n",
              "      <td>Xc</td>\n",
              "      <td>25.58</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>429</th>\n",
              "      <td>[*]O[Si](C)(C)CCCC(=O)Oc1ccc(C=Nc2ccc(N=Cc3ccc...</td>\n",
              "      <td>Xc</td>\n",
              "      <td>29.05</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>430</th>\n",
              "      <td>[*]O[Si](C)(C)CCCC(=O)Oc1ccc(C=Nc2ccc(Cc3ccc(N...</td>\n",
              "      <td>Xc</td>\n",
              "      <td>21.74</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>431</th>\n",
              "      <td>[*]CCN(CCCCCCOc1ccc(C=Cc2ccc([N+](=O)[O-])cc2)...</td>\n",
              "      <td>Xc</td>\n",
              "      <td>7.55</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>432 rows \u00d7 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                smiles property  value  \\\n",
              "0                                              [*]C[*]       Xc  47.80   \n",
              "1                                          [*]CC([*])C       Xc  44.47   \n",
              "2                                         [*]CC([*])CC       Xc  34.04   \n",
              "3                                        [*]CC([*])CCC       Xc  20.01   \n",
              "4                                     [*]CC([*])CC(C)C       Xc  21.64   \n",
              "..                                                 ...      ...    ...   \n",
              "427                                      [*]C([*])(F)F       Xc  31.84   \n",
              "428                                      [*]C/C=C\\C[*]       Xc  25.58   \n",
              "429  [*]O[Si](C)(C)CCCC(=O)Oc1ccc(C=Nc2ccc(N=Cc3ccc...       Xc  29.05   \n",
              "430  [*]O[Si](C)(C)CCCC(=O)Oc1ccc(C=Nc2ccc(Cc3ccc(N...       Xc  21.74   \n",
              "431  [*]CCN(CCCCCCOc1ccc(C=Cc2ccc([N+](=O)[O-])cc2)...       Xc   7.55   \n",
              "\n",
              "                                           fingerprint  \n",
              "0    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
              "1    [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
              "2    [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
              "3    [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
              "4    [0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
              "..                                                 ...  \n",
              "427  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
              "428  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
              "429  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...  \n",
              "430  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...  \n",
              "431  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, ...  \n",
              "\n",
              "[432 rows x 4 columns]"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_ = pd.read_json(\n",
        "    \"https://raw.githubusercontent.com/kuennethgroup/materials_datasets/refs/heads/main/polymer_tendency_to_crystalize/polymers_tend_to_crystalize.json\"\n",
        ")\n",
        "# ... and easy-peasy\n",
        "df_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install scikit-learn autogluon"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prepare data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>value</th>\n",
              "      <th>fingerprint</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>47.80</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>44.47</td>\n",
              "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>34.04</td>\n",
              "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>20.01</td>\n",
              "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>21.64</td>\n",
              "      <td>[0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>427</th>\n",
              "      <td>31.84</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>428</th>\n",
              "      <td>25.58</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>429</th>\n",
              "      <td>29.05</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>430</th>\n",
              "      <td>21.74</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>431</th>\n",
              "      <td>7.55</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>432 rows \u00d7 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     value                                        fingerprint\n",
              "0    47.80  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
              "1    44.47  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
              "2    34.04  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
              "3    20.01  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
              "4    21.64  [0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
              "..     ...                                                ...\n",
              "427  31.84  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
              "428  25.58  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
              "429  29.05  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...\n",
              "430  21.74  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...\n",
              "431   7.55  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, ...\n",
              "\n",
              "[432 rows x 2 columns]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_ = df_.drop(columns=[\"property\", \"smiles\"])\n",
        "df_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>value</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>...</th>\n",
              "      <th>2038</th>\n",
              "      <th>2039</th>\n",
              "      <th>2040</th>\n",
              "      <th>2041</th>\n",
              "      <th>2042</th>\n",
              "      <th>2043</th>\n",
              "      <th>2044</th>\n",
              "      <th>2045</th>\n",
              "      <th>2046</th>\n",
              "      <th>2047</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>47.80</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>44.47</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>34.04</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>20.01</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>21.64</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>427</th>\n",
              "      <td>31.84</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>428</th>\n",
              "      <td>25.58</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>429</th>\n",
              "      <td>29.05</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>430</th>\n",
              "      <td>21.74</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>431</th>\n",
              "      <td>7.55</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>432 rows \u00d7 2049 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     value  0  1  2  3  4  5  6  7  8  ...  2038  2039  2040  2041  2042  \\\n",
              "0    47.80  0  0  0  0  0  0  0  0  0  ...     0     0     0     0     0   \n",
              "1    44.47  0  1  0  0  0  0  0  0  0  ...     0     0     0     0     0   \n",
              "2    34.04  1  1  0  0  0  0  0  0  0  ...     0     0     0     0     0   \n",
              "3    20.01  0  1  0  0  0  0  0  0  0  ...     0     0     0     0     0   \n",
              "4    21.64  0  2  0  0  0  0  0  0  0  ...     0     0     0     0     0   \n",
              "..     ... .. .. .. .. .. .. .. .. ..  ...   ...   ...   ...   ...   ...   \n",
              "427  31.84  0  0  0  0  0  0  0  0  0  ...     0     0     0     0     0   \n",
              "428  25.58  0  0  0  0  0  0  0  0  0  ...     0     0     0     0     0   \n",
              "429  29.05  0  0  0  0  0  0  0  0  0  ...     0     0     0     0     0   \n",
              "430  21.74  0  0  0  0  0  0  0  0  0  ...     0     0     0     0     0   \n",
              "431   7.55  0  0  0  0  0  0  0  0  0  ...     0     0     0     0     0   \n",
              "\n",
              "     2043  2044  2045  2046  2047  \n",
              "0       0     0     0     0     0  \n",
              "1       0     0     0     0     0  \n",
              "2       0     0     0     0     0  \n",
              "3       0     0     0     0     0  \n",
              "4       0     0     0     0     0  \n",
              "..    ...   ...   ...   ...   ...  \n",
              "427     0     0     0     0     0  \n",
              "428     0     0     0     0     0  \n",
              "429     0     0     0     0     0  \n",
              "430     0     0     0     0     0  \n",
              "431     0     0     0     0     1  \n",
              "\n",
              "[432 rows x 2049 columns]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "df = df_[\"value\"].to_frame()\n",
        "df = pd.concat((df, pd.DataFrame(np.vstack(df_[\"fingerprint\"]))), axis=1)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(432, 345, 87)"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df_train, df_test = train_test_split(df, test_size=0.20, random_state=42)\n",
        "len(df), len(df_train), len(df_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>value</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>...</th>\n",
              "      <th>2038</th>\n",
              "      <th>2039</th>\n",
              "      <th>2040</th>\n",
              "      <th>2041</th>\n",
              "      <th>2042</th>\n",
              "      <th>2043</th>\n",
              "      <th>2044</th>\n",
              "      <th>2045</th>\n",
              "      <th>2046</th>\n",
              "      <th>2047</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>132</th>\n",
              "      <td>57.620000</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>231</th>\n",
              "      <td>28.370000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>21.610000</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>84</th>\n",
              "      <td>79.990000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>296</th>\n",
              "      <td>0.370000</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71</th>\n",
              "      <td>29.770000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>106</th>\n",
              "      <td>21.110000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>270</th>\n",
              "      <td>8.900000</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>348</th>\n",
              "      <td>12.490000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102</th>\n",
              "      <td>31.755734</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>345 rows \u00d7 2049 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         value  0  1  2  3  4  5  6  7  8  ...  2038  2039  2040  2041  2042  \\\n",
              "132  57.620000  0  1  0  0  0  0  0  0  0  ...     0     0     0     0     0   \n",
              "231  28.370000  0  0  0  0  0  0  0  0  0  ...     0     0     0     0     0   \n",
              "31   21.610000  0  1  0  0  0  0  0  0  0  ...     0     0     0     0     0   \n",
              "84   79.990000  0  0  0  0  0  0  0  0  0  ...     0     0     0     0     0   \n",
              "296   0.370000  0  1  0  0  0  0  0  0  0  ...     0     0     0     0     0   \n",
              "..         ... .. .. .. .. .. .. .. .. ..  ...   ...   ...   ...   ...   ...   \n",
              "71   29.770000  0  0  0  0  0  0  0  0  0  ...     0     0     0     0     0   \n",
              "106  21.110000  0  0  0  0  0  0  0  0  0  ...     0     0     0     0     0   \n",
              "270   8.900000  0  1  0  0  0  0  0  0  0  ...     0     0     0     0     0   \n",
              "348  12.490000  0  0  0  0  0  0  0  0  0  ...     0     0     0     1     0   \n",
              "102  31.755734  0  0  0  0  0  0  0  0  0  ...     0     0     0     0     0   \n",
              "\n",
              "     2043  2044  2045  2046  2047  \n",
              "132     0     0     0     0     0  \n",
              "231     0     0     0     0     0  \n",
              "31      0     0     0     0     0  \n",
              "84      0     0     0     0     0  \n",
              "296     0     0     0     0     0  \n",
              "..    ...   ...   ...   ...   ...  \n",
              "71      0     0     0     0     0  \n",
              "106     0     0     0     0     0  \n",
              "270     0     0     0     0     0  \n",
              "348     0     0     0     0     0  \n",
              "102     0     0     0     0     0  \n",
              "\n",
              "[345 rows x 2049 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>value</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>...</th>\n",
              "      <th>2038</th>\n",
              "      <th>2039</th>\n",
              "      <th>2040</th>\n",
              "      <th>2041</th>\n",
              "      <th>2042</th>\n",
              "      <th>2043</th>\n",
              "      <th>2044</th>\n",
              "      <th>2045</th>\n",
              "      <th>2046</th>\n",
              "      <th>2047</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>424</th>\n",
              "      <td>8.380000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75</th>\n",
              "      <td>72.530000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>180</th>\n",
              "      <td>53.050000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>16.730000</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>392</th>\n",
              "      <td>38.130000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57</th>\n",
              "      <td>62.840000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>124</th>\n",
              "      <td>88.390000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>12.637895</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>37.020000</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66</th>\n",
              "      <td>22.980000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>87 rows \u00d7 2049 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         value  0  1  2  3  4  5  6  7  8  ...  2038  2039  2040  2041  2042  \\\n",
              "424   8.380000  0  0  0  0  0  0  0  0  0  ...     0     0     0     0     0   \n",
              "75   72.530000  0  0  0  0  0  0  0  0  0  ...     0     0     0     0     0   \n",
              "180  53.050000  0  0  0  0  0  0  0  0  1  ...     0     0     0     0     0   \n",
              "30   16.730000  0  1  0  0  0  0  0  0  0  ...     0     0     0     0     0   \n",
              "392  38.130000  0  0  0  0  0  0  0  0  0  ...     0     0     0     0     0   \n",
              "..         ... .. .. .. .. .. .. .. .. ..  ...   ...   ...   ...   ...   ...   \n",
              "57   62.840000  0  0  0  0  0  0  0  0  0  ...     0     0     0     0     0   \n",
              "124  88.390000  0  0  0  0  0  0  0  0  0  ...     0     0     0     0     0   \n",
              "24   12.637895  0  0  0  0  0  0  0  0  0  ...     0     0     0     0     0   \n",
              "17   37.020000  0  1  0  0  0  0  0  0  0  ...     0     0     0     0     0   \n",
              "66   22.980000  0  0  0  0  0  0  0  0  0  ...     0     0     0     0     0   \n",
              "\n",
              "     2043  2044  2045  2046  2047  \n",
              "424     0     0     0     0     0  \n",
              "75      0     0     0     0     0  \n",
              "180     0     0     0     0     0  \n",
              "30      0     0     0     0     0  \n",
              "392     0     0     0     0     0  \n",
              "..    ...   ...   ...   ...   ...  \n",
              "57      0     0     0     0     0  \n",
              "124     0     0     0     0     0  \n",
              "24      0     0     0     0     0  \n",
              "17      0     0     0     0     0  \n",
              "66      0     0     0     0     0  \n",
              "\n",
              "[87 rows x 2049 columns]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from autogluon.tabular import TabularDataset, TabularPredictor\n",
        "\n",
        "train_data = TabularDataset(df_train)\n",
        "display(train_data)\n",
        "\n",
        "test_data = TabularDataset(df_test)\n",
        "test_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No path specified. Models will be saved in: \"AutogluonModels/ag-20250108_095935\"\n",
            "Verbosity: 2 (Standard Logging)\n",
            "=================== System Info ===================\n",
            "AutoGluon Version:  1.2\n",
            "Python Version:     3.10.12\n",
            "Operating System:   Linux\n",
            "Platform Machine:   x86_64\n",
            "Platform Version:   #140-Ubuntu SMP Wed Dec 18 17:59:53 UTC 2024\n",
            "CPU Count:          192\n",
            "Memory Avail:       992.77 GB / 1007.45 GB (98.5%)\n",
            "Disk Space Avail:   1483.10 GB / 7096.34 GB (20.9%)\n",
            "===================================================\n",
            "Presets specified: ['high_quality']\n",
            "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
            "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
            "Note: `save_bag_folds=False`! This will greatly reduce peak disk usage during fit (by ~8x), but runs the risk of an out-of-memory error during model refit if memory is small relative to the data size.\n",
            "\tYou can avoid this risk by setting `save_bag_folds=True`.\n",
            "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
            "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
            "\tRunning DyStack for up to 50s of the 200s of remaining time (25%).\n",
            "\tRunning DyStack sub-fit in a ray process to avoid memory leakage. Enabling ray logging (enable_ray_logging=True). Specify `ds_args={'enable_ray_logging': False}` if you experience logging issues.\n",
            "2025-01-08 10:59:37,763\tINFO worker.py:1810 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
            "\t\tContext path: \"/home/chris/courses/wt-25-ml-in-ms/wt_25_ml_in_ms/08_ex/AutogluonModels/ag-20250108_095935/ds_sub_fit/sub_fit_ho\"\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m Running DyStack sub-fit ...\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m Beginning AutoGluon training ... Time limit = 47s\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m AutoGluon will save models to \"/home/chris/courses/wt-25-ml-in-ms/wt_25_ml_in_ms/08_ex/AutogluonModels/ag-20250108_095935/ds_sub_fit/sub_fit_ho\"\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m Train Data Rows:    306\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m Train Data Columns: 2048\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m Label Column:       value\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m Problem Type:       regression\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m Preprocessing data ...\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m Using Feature Generators to preprocess the data ...\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m Fitting AutoMLPipelineFeatureGenerator...\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \tAvailable Memory:                    1008859.11 MB\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \tTrain Data (Original)  Memory Usage: 4.78 MB (0.0% of available memory)\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \tStage 1 Generators:\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \t\tFitting AsTypeFeatureGenerator...\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \t\t\tNote: Converting 815 features to boolean dtype as they only contain 2 unique values.\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \tStage 2 Generators:\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \t\tFitting FillNaFeatureGenerator...\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \tStage 3 Generators:\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \t\tFitting IdentityFeatureGenerator...\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \tStage 4 Generators:\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \t\tFitting DropUniqueFeatureGenerator...\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \tStage 5 Generators:\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \tUseless Original Features (Count: 824): ['0', '3', '6', '10', '12', '15', '16', '18', '19', '20', '21', '23', '28', '35', '38', '40', '42', '46', '51', '56', '60', '61', '62', '63', '64', '65', '66', '71', '72', '77', '78', '82', '85', '86', '88', '89', '91', '92', '95', '96', '101', '103', '104', '107', '108', '111', '112', '113', '119', '120', '121', '123', '124', '127', '128', '129', '130', '134', '136', '137', '138', '141', '149', '150', '151', '153', '154', '155', '159', '160', '161', '165', '168', '169', '172', '177', '178', '179', '181', '182', '186', '189', '194', '198', '200', '205', '207', '208', '211', '215', '216', '218', '223', '224', '229', '238', '241', '242', '244', '250', '256', '257', '258', '259', '262', '263', '266', '267', '268', '272', '274', '277', '278', '280', '284', '286', '290', '291', '296', '297', '298', '299', '300', '309', '313', '320', '321', '324', '327', '328', '329', '331', '332', '334', '335', '337', '338', '344', '345', '346', '349', '351', '356', '360', '362', '365', '369', '370', '371', '380', '382', '384', '388', '390', '394', '395', '396', '397', '400', '402', '403', '404', '405', '409', '410', '412', '413', '414', '415', '416', '417', '418', '419', '421', '425', '426', '427', '431', '433', '434', '435', '439', '441', '442', '446', '447', '448', '450', '452', '454', '455', '456', '457', '458', '459', '462', '464', '465', '468', '470', '474', '475', '479', '491', '492', '493', '496', '500', '501', '505', '506', '508', '513', '514', '517', '520', '523', '524', '528', '529', '530', '531', '532', '533', '535', '536', '542', '543', '546', '550', '551', '552', '554', '556', '557', '558', '560', '565', '566', '571', '575', '576', '586', '593', '595', '596', '601', '602', '610', '611', '613', '614', '616', '623', '624', '627', '630', '631', '634', '635', '638', '641', '644', '646', '647', '651', '652', '657', '661', '665', '669', '672', '678', '688', '689', '690', '696', '697', '700', '702', '704', '705', '708', '709', '712', '716', '717', '720', '722', '724', '727', '729', '730', '731', '732', '735', '737', '738', '740', '747', '748', '752', '754', '756', '757', '758', '762', '764', '765', '768', '771', '773', '777', '778', '780', '782', '786', '788', '791', '792', '793', '795', '800', '805', '808', '809', '813', '815', '816', '817', '818', '819', '821', '833', '837', '839', '840', '842', '844', '847', '849', '852', '855', '861', '869', '872', '873', '874', '876', '884', '885', '887', '889', '890', '894', '899', '902', '903', '911', '913', '917', '919', '920', '922', '927', '928', '930', '938', '939', '943', '944', '947', '949', '950', '952', '955', '958', '962', '963', '965', '970', '971', '973', '974', '976', '977', '978', '980', '982', '985', '988', '989', '990', '992', '993', '997', '1000', '1001', '1002', '1005', '1011', '1016', '1018', '1021', '1022', '1025', '1026', '1029', '1030', '1033', '1035', '1037', '1040', '1041', '1046', '1047', '1053', '1058', '1059', '1065', '1072', '1073', '1075', '1078', '1079', '1086', '1092', '1093', '1094', '1097', '1099', '1101', '1102', '1104', '1105', '1106', '1112', '1115', '1121', '1122', '1124', '1129', '1130', '1131', '1133', '1134', '1136', '1137', '1139', '1141', '1148', '1149', '1150', '1153', '1154', '1159', '1166', '1169', '1170', '1171', '1177', '1181', '1183', '1186', '1190', '1192', '1193', '1194', '1198', '1202', '1204', '1206', '1207', '1211', '1212', '1214', '1216', '1217', '1218', '1220', '1226', '1228', '1230', '1231', '1233', '1236', '1237', '1241', '1242', '1246', '1247', '1250', '1253', '1255', '1256', '1257', '1258', '1259', '1260', '1261', '1264', '1265', '1268', '1270', '1271', '1272', '1273', '1275', '1279', '1281', '1284', '1285', '1288', '1291', '1293', '1294', '1295', '1298', '1300', '1302', '1304', '1305', '1307', '1311', '1312', '1315', '1316', '1320', '1329', '1330', '1334', '1336', '1337', '1338', '1339', '1340', '1342', '1346', '1351', '1354', '1355', '1358', '1359', '1361', '1363', '1367', '1368', '1369', '1371', '1373', '1376', '1377', '1381', '1387', '1388', '1389', '1390', '1395', '1396', '1397', '1401', '1402', '1403', '1404', '1405', '1406', '1407', '1408', '1410', '1412', '1414', '1418', '1423', '1426', '1428', '1429', '1431', '1433', '1434', '1437', '1438', '1443', '1447', '1448', '1449', '1451', '1455', '1458', '1461', '1465', '1468', '1469', '1474', '1475', '1477', '1478', '1483', '1484', '1486', '1488', '1491', '1496', '1497', '1498', '1500', '1504', '1505', '1509', '1510', '1511', '1512', '1519', '1521', '1526', '1528', '1529', '1532', '1535', '1537', '1538', '1539', '1540', '1541', '1543', '1545', '1547', '1548', '1549', '1550', '1552', '1554', '1556', '1558', '1560', '1561', '1562', '1563', '1566', '1567', '1568', '1570', '1572', '1574', '1577', '1578', '1583', '1584', '1591', '1593', '1595', '1596', '1597', '1598', '1600', '1601', '1613', '1615', '1620', '1621', '1623', '1626', '1628', '1629', '1630', '1631', '1635', '1636', '1637', '1638', '1640', '1642', '1644', '1648', '1651', '1653', '1657', '1662', '1663', '1664', '1667', '1670', '1675', '1677', '1678', '1680', '1681', '1682', '1684', '1687', '1689', '1690', '1696', '1698', '1701', '1702', '1704', '1706', '1707', '1709', '1711', '1716', '1723', '1724', '1725', '1730', '1732', '1733', '1736', '1739', '1741', '1742', '1744', '1748', '1751', '1752', '1756', '1759', '1761', '1763', '1765', '1767', '1771', '1776', '1780', '1783', '1784', '1787', '1789', '1790', '1792', '1794', '1797', '1801', '1808', '1813', '1815', '1817', '1818', '1820', '1828', '1832', '1833', '1835', '1836', '1837', '1838', '1841', '1852', '1859', '1861', '1864', '1866', '1869', '1871', '1872', '1874', '1875', '1879', '1880', '1881', '1890', '1891', '1892', '1893', '1897', '1902', '1903', '1906', '1907', '1909', '1912', '1914', '1918', '1921', '1927', '1931', '1933', '1936', '1938', '1943', '1944', '1946', '1949', '1953', '1954', '1957', '1961', '1966', '1967', '1969', '1972', '1973', '1974', '1976', '1977', '1980', '1981', '1986', '1987', '1994', '1997', '1998', '1999', '2005', '2007', '2008', '2010', '2011', '2012', '2014', '2015', '2017', '2018', '2024', '2025', '2028', '2029', '2030', '2037', '2038', '2040', '2042', '2044', '2046']\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \t\tThis is typically a feature which has the same value for all rows.\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \t\tThese features do not need to be present at inference time.\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \tUnused Original Features (Count: 382): ['30', '47', '67', '75', '146', '157', '163', '185', '199', '201', '204', '234', '246', '254', '255', '260', '265', '271', '285', '301', '304', '305', '306', '307', '315', '317', '318', '341', '347', '353', '364', '372', '377', '381', '387', '406', '411', '428', '445', '451', '472', '485', '490', '498', '499', '502', '509', '512', '516', '525', '527', '537', '538', '544', '549', '561', '563', '567', '568', '582', '585', '599', '600', '603', '629', '633', '643', '648', '649', '653', '654', '655', '658', '659', '663', '666', '675', '681', '701', '707', '719', '723', '726', '741', '750', '761', '767', '772', '774', '776', '779', '796', '797', '798', '802', '810', '811', '812', '814', '822', '825', '828', '838', '845', '853', '860', '863', '871', '877', '880', '895', '897', '904', '906', '914', '916', '925', '936', '941', '946', '954', '960', '966', '968', '979', '984', '986', '1004', '1020', '1027', '1032', '1042', '1048', '1049', '1050', '1051', '1056', '1060', '1061', '1062', '1067', '1068', '1069', '1076', '1080', '1082', '1083', '1084', '1085', '1089', '1091', '1095', '1098', '1100', '1108', '1109', '1111', '1118', '1123', '1125', '1126', '1127', '1128', '1138', '1140', '1142', '1155', '1165', '1167', '1168', '1172', '1173', '1175', '1184', '1187', '1189', '1195', '1197', '1203', '1208', '1210', '1215', '1219', '1222', '1223', '1225', '1235', '1239', '1245', '1248', '1262', '1266', '1278', '1289', '1296', '1306', '1309', '1310', '1322', '1323', '1324', '1331', '1332', '1333', '1335', '1347', '1348', '1353', '1357', '1360', '1362', '1372', '1374', '1378', '1379', '1383', '1393', '1400', '1409', '1411', '1413', '1415', '1416', '1421', '1424', '1439', '1445', '1457', '1463', '1464', '1467', '1470', '1472', '1479', '1481', '1487', '1489', '1490', '1492', '1499', '1503', '1506', '1508', '1514', '1515', '1516', '1517', '1523', '1527', '1530', '1531', '1533', '1534', '1542', '1553', '1557', '1559', '1569', '1573', '1576', '1580', '1585', '1604', '1605', '1606', '1607', '1608', '1609', '1611', '1614', '1616', '1618', '1619', '1624', '1625', '1627', '1633', '1634', '1639', '1646', '1650', '1654', '1656', '1659', '1672', '1674', '1685', '1692', '1708', '1710', '1712', '1713', '1720', '1721', '1726', '1735', '1738', '1745', '1746', '1749', '1760', '1762', '1764', '1773', '1774', '1778', '1779', '1781', '1782', '1785', '1786', '1791', '1793', '1796', '1802', '1803', '1805', '1807', '1812', '1814', '1819', '1827', '1829', '1834', '1844', '1845', '1848', '1850', '1858', '1860', '1862', '1863', '1865', '1867', '1868', '1870', '1877', '1883', '1889', '1894', '1898', '1899', '1900', '1901', '1904', '1905', '1908', '1913', '1919', '1922', '1923', '1925', '1929', '1932', '1935', '1937', '1939', '1941', '1950', '1955', '1960', '1965', '1968', '1971', '1975', '1983', '1984', '1993', '2001', '2002', '2003', '2006', '2013', '2021', '2022', '2026', '2031', '2032', '2034', '2039', '2043', '2045']\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \t\tThese features do not need to be present at inference time.\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \t\t('int', []) : 382 | ['30', '47', '67', '75', '146', ...]\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \tTypes of features in original data (raw dtype, special dtypes):\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \t\t('int', []) : 842 | ['1', '2', '4', '5', '7', ...]\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \t\t('int', [])       : 389 | ['1', '8', '9', '11', '13', ...]\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \t\t('int', ['bool']) : 453 | ['2', '4', '5', '7', '14', ...]\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \t2.4s = Fit runtime\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \t842 features in original data used to generate 842 features in processed data.\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \tTrain Data (Processed) Memory Usage: 1.04 MB (0.0% of available memory)\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m Data preprocessing and feature engineering runtime = 2.42s ...\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \tTo change this, specify the eval_metric parameter of Predictor()\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m User-specified model hyperparameters to be fit:\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m {\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m }\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m Fitting 108 L1 models, fit_strategy=\"sequential\" ...\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 29.66s of the 44.50s of remaining time.\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \t-21.769\t = Validation score   (-root_mean_squared_error)\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \t0.03s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \t0.28s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 29.26s of the 44.10s of remaining time.\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \t-21.2139\t = Validation score   (-root_mean_squared_error)\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \t0.03s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \t0.26s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 28.87s of the 43.71s of remaining time.\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.02%)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(_ray_fit pid=231062)\u001b[0m [1000]\tvalid_set's rmse: 13.4164\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(_dystack pid=209618)\u001b[0m \t-18.6573\t = Validation score   (-root_mean_squared_error)\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \t1.06s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \t0.03s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m Fitting model: LightGBM_BAG_L1 ... Training model for up to 26.50s of the 41.33s of remaining time.\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.02%)\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \t-19.1182\t = Validation score   (-root_mean_squared_error)\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \t1.01s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \t0.03s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 24.19s of the 39.03s of remaining time.\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \t-19.3923\t = Validation score   (-root_mean_squared_error)\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \t0.68s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \t0.09s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m Fitting model: CatBoost_BAG_L1 ... Training model for up to 23.30s of the 38.13s of remaining time.\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.03%)\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \t-19.1311\t = Validation score   (-root_mean_squared_error)\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \t2.6s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \t0.25s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m Fitting model: ExtraTreesMSE_BAG_L1 ... Training model for up to 19.32s of the 34.16s of remaining time.\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \t-19.4966\t = Validation score   (-root_mean_squared_error)\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \t0.48s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \t0.1s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 18.62s of the 33.46s of remaining time.\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
            "\u001b[36m(_ray_fit pid=237759)\u001b[0m No improvement since epoch 7: early stopping\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \t-20.1716\t = Validation score   (-root_mean_squared_error)\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \t3.31s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \t0.06s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m Fitting model: XGBoost_BAG_L1 ... Training model for up to 13.99s of the 28.82s of remaining time.\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.03%)\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \t-19.6345\t = Validation score   (-root_mean_squared_error)\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \t1.36s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \t0.03s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 11.31s of the 26.14s of remaining time.\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
            "\u001b[36m(_ray_fit pid=241247)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 271)\n",
            "\u001b[36m(_ray_fit pid=237758)\u001b[0m No improvement since epoch 7: early stopping\u001b[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \t-17.8295\t = Validation score   (-root_mean_squared_error)\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \t10.45s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \t1.09s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m Fitting model: WeightedEnsemble_L2 ... Training model for up to 44.51s of the 14.23s of remaining time.\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \tEnsemble Weights: {'NeuralNetTorch_BAG_L1': 0.68, 'LightGBMXT_BAG_L1': 0.28, 'RandomForestMSE_BAG_L1': 0.04}\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \t-17.6116\t = Validation score   (-root_mean_squared_error)\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \t0.01s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \t0.0s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m Fitting 106 L2 models, fit_strategy=\"sequential\" ...\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 14.18s of the 14.16s of remaining time.\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.02%)\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \t-18.164\t = Validation score   (-root_mean_squared_error)\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \t0.91s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \t0.03s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m Fitting model: LightGBM_BAG_L2 ... Training model for up to 11.88s of the 11.86s of remaining time.\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.02%)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(_ray_fit pid=244999)\u001b[0m [1000]\tvalid_set's rmse: 16.6547\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(_dystack pid=209618)\u001b[0m \t-18.1495\t = Validation score   (-root_mean_squared_error)\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \t0.93s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \t0.04s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m Fitting model: RandomForestMSE_BAG_L2 ... Training model for up to 9.57s of the 9.55s of remaining time.\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \t-18.5531\t = Validation score   (-root_mean_squared_error)\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \t0.56s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \t0.12s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m Fitting model: CatBoost_BAG_L2 ... Training model for up to 8.75s of the 8.73s of remaining time.\n",
            "\u001b[36m(_ray_fit pid=241241)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 286)\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.03%)\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \t-18.0756\t = Validation score   (-root_mean_squared_error)\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \t2.19s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \t0.22s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m Fitting model: ExtraTreesMSE_BAG_L2 ... Training model for up to 5.11s of the 5.08s of remaining time.\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \t-18.4088\t = Validation score   (-root_mean_squared_error)\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \t0.51s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \t0.12s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m Fitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 4.36s of the 4.33s of remaining time.\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
            "\u001b[36m(_ray_fit pid=250324)\u001b[0m No improvement since epoch 6: early stopping\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \t-19.4626\t = Validation score   (-root_mean_squared_error)\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \t3.37s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \t0.07s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m Fitting model: WeightedEnsemble_L3 ... Training model for up to 44.51s of the -0.47s of remaining time.\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \tEnsemble Weights: {'NeuralNetTorch_BAG_L1': 0.522, 'LightGBM_BAG_L2': 0.348, 'NeuralNetFastAI_BAG_L2': 0.087, 'CatBoost_BAG_L2': 0.043}\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \t-17.5068\t = Validation score   (-root_mean_squared_error)\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \t0.01s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \t0.0s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m AutoGluon training complete, total runtime = 47.46s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 21.1 rows/s (39 batch size)\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m Automatically performing refit_full as a post-fit operation (due to `.fit(..., refit_full=True)`\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m Refitting models via `predictor.refit_full` using all of the data (combined train and validation)...\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \tModels trained in this way will have the suffix \"_FULL\" and have NaN validation score.\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \tThis process is not bound by time_limit, but should take less time than the original `predictor.fit` call.\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \tTo learn more, refer to the `.refit_full` method docstring which explains how \"_FULL\" models differ from normal models.\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m Fitting model: KNeighborsUnif_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \t0.03s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \t0.28s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m Fitting model: KNeighborsDist_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \t0.03s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \t0.26s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m Fitting model: LightGBMXT_BAG_L1_FULL ...\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \t0.7s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m Fitting model: LightGBM_BAG_L1_FULL ...\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \t0.65s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m Fitting model: RandomForestMSE_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \t0.68s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \t0.09s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m Fitting model: CatBoost_BAG_L1_FULL ...\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \t3.44s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m Fitting model: ExtraTreesMSE_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \t0.48s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \t0.1s\t = Validation runtime\n",
            "\u001b[36m(_ray_fit pid=250330)\u001b[0m No improvement since epoch 7: early stopping\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m Fitting model: NeuralNetFastAI_BAG_L1_FULL ...\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \tStopping at the best epoch learned earlier - 12.\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \t0.75s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m Fitting model: XGBoost_BAG_L1_FULL ...\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \t0.39s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m Fitting model: NeuralNetTorch_BAG_L1_FULL ...\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \t7.77s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m Fitting model: WeightedEnsemble_L2_FULL | Skipping fit via cloning parent ...\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \tEnsemble Weights: {'NeuralNetTorch_BAG_L1': 0.68, 'LightGBMXT_BAG_L1': 0.28, 'RandomForestMSE_BAG_L1': 0.04}\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \t0.01s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m Fitting 1 L2 models, fit_strategy=\"sequential\" ...\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m Fitting model: LightGBMXT_BAG_L2_FULL ...\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \t0.59s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m Fitting 1 L2 models, fit_strategy=\"sequential\" ...\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m Fitting model: LightGBM_BAG_L2_FULL ...\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \t0.65s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m Fitting model: RandomForestMSE_BAG_L2_FULL | Skipping fit via cloning parent ...\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \t0.56s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \t0.12s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m Fitting 1 L2 models, fit_strategy=\"sequential\" ...\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m Fitting model: CatBoost_BAG_L2_FULL ...\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \t0.87s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m Fitting model: ExtraTreesMSE_BAG_L2_FULL | Skipping fit via cloning parent ...\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \t0.51s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \t0.12s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m Fitting 1 L2 models, fit_strategy=\"sequential\" ...\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m Fitting model: NeuralNetFastAI_BAG_L2_FULL ...\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \tStopping at the best epoch learned earlier - 13.\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \t0.6s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m Fitting model: WeightedEnsemble_L3_FULL | Skipping fit via cloning parent ...\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \tEnsemble Weights: {'NeuralNetTorch_BAG_L1': 0.522, 'LightGBM_BAG_L2': 0.348, 'NeuralNetFastAI_BAG_L2': 0.087, 'CatBoost_BAG_L2': 0.043}\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m \t0.01s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m Updated best model to \"WeightedEnsemble_L3_FULL\" (Previously \"WeightedEnsemble_L3\"). AutoGluon will default to using \"WeightedEnsemble_L3_FULL\" for predict() and predict_proba().\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m Refit complete, total runtime = 19.22s ... Best model: \"WeightedEnsemble_L3_FULL\"\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/home/chris/courses/wt-25-ml-in-ms/wt_25_ml_in_ms/08_ex/AutogluonModels/ag-20250108_095935/ds_sub_fit/sub_fit_ho\")\n",
            "\u001b[36m(_dystack pid=209618)\u001b[0m Deleting DyStack predictor artifacts (clean_up_fits=True) ...\n",
            "Leaderboard on holdout data (DyStack):\n",
            "                          model  score_holdout  score_val              eval_metric  pred_time_test  pred_time_val   fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
            "0     ExtraTreesMSE_BAG_L2_FULL     -13.035189 -18.408831  root_mean_squared_error        0.690292            NaN  14.749389                 0.065631                0.119691           0.505683            2       True         16\n",
            "1    KNeighborsUnif_BAG_L1_FULL     -13.659472 -21.769039  root_mean_squared_error        0.216345       0.281132   0.029869                 0.216345                0.281132           0.029869            1       True          1\n",
            "2    KNeighborsDist_BAG_L1_FULL     -13.849166 -21.213949  root_mean_squared_error        0.276686       0.263577   0.032132                 0.276686                0.263577           0.032132            1       True          2\n",
            "3        LightGBMXT_BAG_L1_FULL     -14.094939 -18.657271  root_mean_squared_error        0.020807            NaN   0.701706                 0.020807                     NaN           0.701706            1       True          3\n",
            "4          LightGBM_BAG_L1_FULL     -14.178505 -19.118233  root_mean_squared_error        0.029702            NaN   0.648421                 0.029702                     NaN           0.648421            1       True          4\n",
            "5          CatBoost_BAG_L2_FULL     -14.245887 -18.075572  root_mean_squared_error        0.820244            NaN  15.117919                 0.195583                     NaN           0.874213            2       True         15\n",
            "6   RandomForestMSE_BAG_L2_FULL     -14.300544 -18.553085  root_mean_squared_error        0.737879            NaN  14.808566                 0.113217                0.122194           0.564860            2       True         14\n",
            "7        LightGBMXT_BAG_L2_FULL     -14.334403 -18.164021  root_mean_squared_error        0.645187            NaN  14.830397                 0.020525                     NaN           0.586691            2       True         12\n",
            "8          CatBoost_BAG_L1_FULL     -14.671804 -19.131090  root_mean_squared_error        0.023180            NaN   3.437693                 0.023180                     NaN           3.437693            1       True          6\n",
            "9      WeightedEnsemble_L2_FULL     -15.000125 -17.611588  root_mean_squared_error        0.196712            NaN   9.162294                 0.001657                     NaN           0.008575            2       True         11\n",
            "10          XGBoost_BAG_L1_FULL     -15.062723 -19.634492  root_mean_squared_error        0.046850            NaN   0.391901                 0.046850                     NaN           0.391901            1       True          9\n",
            "11     WeightedEnsemble_L3_FULL     -15.203567 -17.506806  root_mean_squared_error        0.863326            NaN  16.374136                 0.001873                     NaN           0.008756            3       True         18\n",
            "12         LightGBM_BAG_L2_FULL     -15.239660 -18.149536  root_mean_squared_error        0.648912            NaN  14.890538                 0.024251                     NaN           0.646832            2       True         13\n",
            "13  NeuralNetFastAI_BAG_L1_FULL     -15.311333 -20.171628  root_mean_squared_error        0.020109            NaN   0.748808                 0.020109                     NaN           0.748808            1       True          8\n",
            "14    ExtraTreesMSE_BAG_L1_FULL     -16.160174 -19.496609  root_mean_squared_error        0.062780       0.102930   0.479452                 0.062780                0.102930           0.479452            1       True          7\n",
            "15  RandomForestMSE_BAG_L1_FULL     -16.164140 -19.392323  root_mean_squared_error        0.062677       0.090237   0.680931                 0.062677                0.090237           0.680931            1       True          5\n",
            "16   NeuralNetTorch_BAG_L1_FULL     -16.858652 -17.829501  root_mean_squared_error        0.111572            NaN   7.771082                 0.111572                     NaN           7.771082            1       True         10\n",
            "17  NeuralNetFastAI_BAG_L2_FULL     -18.095906 -19.462571  root_mean_squared_error        0.641620            NaN  14.844334                 0.016958                     NaN           0.600628            2       True         17\n",
            "\t0\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: True)\n",
            "\t72s\t = DyStack   runtime |\t128s\t = Remaining runtime\n",
            "Starting main fit with num_stack_levels=0.\n",
            "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=0)`\n",
            "Beginning AutoGluon training ... Time limit = 128s\n",
            "AutoGluon will save models to \"/home/chris/courses/wt-25-ml-in-ms/wt_25_ml_in_ms/08_ex/AutogluonModels/ag-20250108_095935\"\n",
            "Train Data Rows:    345\n",
            "Train Data Columns: 2048\n",
            "Label Column:       value\n",
            "Problem Type:       regression\n",
            "Preprocessing data ...\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    1013508.52 MB\n",
            "\tTrain Data (Original)  Memory Usage: 5.39 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 843 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tUseless Original Features (Count: 780): ['3', '6', '10', '15', '16', '18', '19', '21', '23', '28', '35', '38', '40', '42', '46', '51', '56', '60', '62', '63', '64', '65', '66', '71', '72', '77', '78', '82', '85', '86', '88', '89', '91', '92', '95', '96', '101', '103', '104', '107', '108', '111', '112', '113', '119', '120', '121', '123', '124', '127', '128', '129', '130', '134', '136', '137', '138', '141', '149', '150', '151', '153', '154', '155', '159', '160', '161', '168', '169', '172', '177', '178', '179', '181', '182', '186', '189', '194', '198', '200', '205', '207', '211', '215', '216', '218', '223', '224', '229', '238', '241', '242', '244', '250', '256', '258', '259', '263', '266', '268', '272', '274', '277', '278', '280', '284', '286', '290', '291', '296', '297', '298', '299', '300', '309', '313', '321', '327', '328', '329', '331', '332', '334', '335', '337', '338', '344', '345', '346', '349', '351', '356', '360', '362', '365', '369', '370', '371', '380', '382', '384', '388', '390', '394', '395', '396', '397', '400', '402', '403', '404', '405', '409', '410', '412', '413', '414', '415', '416', '417', '418', '419', '421', '425', '426', '427', '431', '433', '434', '435', '439', '441', '442', '446', '447', '448', '450', '452', '454', '455', '456', '457', '458', '459', '462', '464', '465', '468', '470', '474', '475', '479', '491', '492', '493', '496', '500', '501', '505', '506', '508', '513', '514', '517', '520', '523', '524', '528', '529', '531', '532', '533', '535', '536', '542', '543', '546', '550', '551', '552', '554', '556', '557', '560', '565', '566', '571', '575', '576', '586', '593', '595', '596', '601', '602', '610', '611', '613', '614', '616', '623', '624', '627', '630', '631', '634', '635', '641', '646', '647', '651', '661', '665', '669', '672', '678', '688', '689', '690', '696', '697', '700', '702', '704', '705', '708', '709', '712', '716', '717', '720', '722', '724', '727', '730', '731', '732', '735', '737', '738', '740', '747', '748', '752', '754', '756', '757', '758', '762', '765', '768', '771', '773', '777', '778', '780', '782', '786', '788', '791', '792', '793', '795', '800', '805', '808', '809', '813', '815', '816', '817', '818', '819', '821', '833', '837', '840', '842', '844', '847', '849', '852', '855', '861', '869', '872', '873', '874', '876', '884', '885', '887', '889', '890', '894', '899', '902', '903', '911', '913', '917', '919', '920', '922', '927', '928', '930', '938', '939', '943', '944', '947', '949', '950', '952', '955', '958', '962', '963', '965', '970', '971', '973', '974', '976', '977', '978', '980', '982', '985', '988', '989', '990', '992', '997', '1000', '1001', '1002', '1005', '1011', '1016', '1018', '1021', '1022', '1025', '1026', '1029', '1030', '1033', '1035', '1037', '1040', '1041', '1046', '1047', '1053', '1058', '1059', '1065', '1072', '1073', '1075', '1078', '1079', '1086', '1092', '1093', '1094', '1097', '1099', '1101', '1102', '1104', '1105', '1106', '1112', '1115', '1121', '1122', '1124', '1129', '1130', '1131', '1133', '1134', '1136', '1137', '1139', '1141', '1148', '1149', '1150', '1153', '1154', '1159', '1166', '1169', '1170', '1171', '1177', '1181', '1183', '1186', '1190', '1192', '1194', '1198', '1202', '1204', '1206', '1207', '1211', '1212', '1214', '1216', '1217', '1218', '1220', '1226', '1230', '1231', '1233', '1236', '1237', '1241', '1242', '1246', '1247', '1250', '1253', '1256', '1257', '1258', '1259', '1260', '1261', '1264', '1265', '1268', '1270', '1272', '1273', '1275', '1279', '1281', '1284', '1285', '1288', '1291', '1293', '1294', '1295', '1298', '1300', '1302', '1304', '1305', '1307', '1311', '1312', '1315', '1316', '1320', '1329', '1334', '1336', '1337', '1338', '1339', '1340', '1342', '1346', '1351', '1354', '1355', '1358', '1359', '1361', '1367', '1368', '1369', '1371', '1373', '1376', '1377', '1381', '1387', '1388', '1389', '1390', '1395', '1396', '1397', '1401', '1402', '1403', '1404', '1405', '1406', '1407', '1408', '1410', '1412', '1414', '1418', '1423', '1426', '1428', '1429', '1431', '1433', '1434', '1437', '1438', '1443', '1447', '1448', '1449', '1451', '1458', '1461', '1465', '1468', '1469', '1474', '1475', '1477', '1478', '1483', '1484', '1486', '1488', '1491', '1496', '1497', '1498', '1500', '1504', '1505', '1509', '1510', '1511', '1519', '1526', '1528', '1529', '1532', '1535', '1537', '1538', '1541', '1543', '1547', '1548', '1549', '1550', '1552', '1554', '1556', '1558', '1560', '1561', '1562', '1563', '1566', '1567', '1568', '1570', '1572', '1574', '1577', '1578', '1583', '1584', '1591', '1593', '1595', '1596', '1598', '1600', '1601', '1613', '1615', '1620', '1621', '1623', '1626', '1628', '1629', '1631', '1635', '1636', '1637', '1638', '1640', '1642', '1644', '1648', '1651', '1657', '1662', '1663', '1664', '1667', '1670', '1675', '1677', '1678', '1680', '1681', '1682', '1684', '1687', '1689', '1690', '1696', '1698', '1701', '1702', '1704', '1706', '1707', '1709', '1711', '1716', '1723', '1724', '1730', '1732', '1736', '1739', '1742', '1744', '1748', '1752', '1756', '1759', '1761', '1763', '1765', '1767', '1771', '1776', '1780', '1783', '1784', '1787', '1789', '1790', '1792', '1794', '1797', '1801', '1808', '1813', '1815', '1817', '1818', '1820', '1828', '1832', '1833', '1835', '1836', '1837', '1838', '1841', '1852', '1859', '1861', '1864', '1866', '1869', '1871', '1872', '1874', '1875', '1879', '1880', '1881', '1890', '1891', '1892', '1893', '1897', '1902', '1903', '1906', '1907', '1909', '1914', '1918', '1921', '1927', '1931', '1933', '1936', '1938', '1943', '1944', '1946', '1949', '1953', '1954', '1957', '1966', '1967', '1969', '1972', '1973', '1974', '1976', '1977', '1980', '1981', '1986', '1987', '1994', '1998', '1999', '2005', '2007', '2010', '2011', '2012', '2014', '2015', '2017', '2018', '2024', '2025', '2028', '2029', '2030', '2037', '2038', '2040', '2042', '2044', '2046']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tUnused Original Features (Count: 386): ['47', '61', '67', '75', '146', '157', '163', '185', '199', '201', '204', '208', '234', '246', '254', '255', '257', '260', '262', '265', '271', '285', '304', '305', '306', '315', '317', '318', '320', '324', '341', '353', '364', '372', '377', '381', '406', '428', '445', '451', '472', '485', '490', '498', '499', '502', '509', '512', '516', '525', '527', '537', '538', '549', '558', '561', '563', '567', '568', '582', '585', '599', '600', '603', '629', '633', '638', '643', '644', '648', '649', '653', '654', '655', '657', '658', '659', '663', '666', '675', '681', '701', '707', '723', '726', '729', '741', '750', '764', '767', '772', '774', '776', '779', '796', '797', '798', '802', '810', '811', '812', '822', '825', '828', '838', '839', '845', '853', '860', '863', '871', '877', '880', '895', '897', '904', '906', '914', '916', '925', '936', '941', '946', '960', '966', '968', '979', '984', '986', '993', '1004', '1020', '1027', '1032', '1042', '1048', '1049', '1050', '1051', '1060', '1061', '1062', '1067', '1068', '1069', '1080', '1082', '1084', '1085', '1089', '1091', '1095', '1098', '1100', '1108', '1109', '1111', '1118', '1123', '1125', '1126', '1127', '1140', '1142', '1155', '1165', '1167', '1168', '1172', '1173', '1175', '1184', '1187', '1189', '1193', '1195', '1197', '1203', '1208', '1210', '1215', '1219', '1222', '1223', '1225', '1228', '1235', '1239', '1245', '1248', '1266', '1271', '1278', '1289', '1296', '1306', '1310', '1322', '1324', '1330', '1331', '1332', '1333', '1335', '1347', '1348', '1353', '1360', '1362', '1363', '1372', '1374', '1378', '1379', '1383', '1393', '1400', '1409', '1411', '1413', '1416', '1421', '1424', '1439', '1445', '1455', '1457', '1463', '1464', '1467', '1470', '1472', '1479', '1481', '1487', '1489', '1490', '1492', '1503', '1506', '1508', '1512', '1514', '1515', '1516', '1517', '1521', '1523', '1527', '1530', '1531', '1534', '1540', '1542', '1545', '1553', '1557', '1559', '1569', '1573', '1576', '1580', '1585', '1597', '1604', '1605', '1606', '1607', '1608', '1609', '1611', '1614', '1618', '1619', '1624', '1625', '1627', '1630', '1633', '1634', '1639', '1646', '1650', '1653', '1654', '1656', '1659', '1672', '1674', '1685', '1692', '1708', '1710', '1712', '1713', '1720', '1721', '1725', '1726', '1733', '1735', '1738', '1741', '1745', '1746', '1751', '1760', '1762', '1764', '1773', '1778', '1779', '1781', '1782', '1786', '1791', '1793', '1796', '1803', '1805', '1807', '1812', '1814', '1819', '1827', '1829', '1834', '1844', '1845', '1848', '1850', '1858', '1860', '1862', '1863', '1865', '1867', '1868', '1870', '1877', '1883', '1894', '1898', '1899', '1900', '1901', '1904', '1905', '1908', '1912', '1913', '1919', '1922', '1923', '1925', '1929', '1932', '1935', '1937', '1939', '1941', '1950', '1955', '1960', '1961', '1965', '1968', '1971', '1975', '1983', '1993', '1997', '2001', '2003', '2006', '2008', '2013', '2021', '2022', '2026', '2031', '2032', '2034', '2039', '2043', '2045']\n",
            "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
            "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\t\t('int', []) : 386 | ['47', '61', '67', '75', '146', ...]\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('int', []) : 882 | ['0', '1', '2', '4', '5', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('int', [])       : 409 | ['1', '2', '8', '9', '11', ...]\n",
            "\t\t('int', ['bool']) : 473 | ['0', '4', '5', '7', '12', ...]\n",
            "\t2.5s = Fit runtime\n",
            "\t882 features in original data used to generate 882 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 1.23 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 2.58s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
            "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
            "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
            "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Fitting 108 L1 models, fit_strategy=\"sequential\" ...\n",
            "Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 125.09s of the 125.08s of remaining time.\n",
            "\t-20.2691\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.03s\t = Training   runtime\n",
            "\t0.32s\t = Validation runtime\n",
            "Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 124.64s of the 124.64s of remaining time.\n",
            "\t-19.8639\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.03s\t = Training   runtime\n",
            "\t0.18s\t = Validation runtime\n",
            "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 124.36s of the 124.35s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.02%)\n",
            "\t-18.1373\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.98s\t = Training   runtime\n",
            "\t0.03s\t = Validation runtime\n",
            "Fitting model: LightGBM_BAG_L1 ... Training model for up to 122.02s of the 122.01s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.02%)\n",
            "\t-18.6287\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.95s\t = Training   runtime\n",
            "\t0.03s\t = Validation runtime\n",
            "Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 119.67s of the 119.66s of remaining time.\n",
            "\t-19.0034\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.62s\t = Training   runtime\n",
            "\t0.1s\t = Validation runtime\n",
            "Fitting model: CatBoost_BAG_L1 ... Training model for up to 118.82s of the 118.82s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.03%)\n",
            "\t-18.315\t = Validation score   (-root_mean_squared_error)\n",
            "\t4.05s\t = Training   runtime\n",
            "\t0.25s\t = Validation runtime\n",
            "Fitting model: ExtraTreesMSE_BAG_L1 ... Training model for up to 113.38s of the 113.37s of remaining time.\n",
            "\t-19.0719\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.58s\t = Training   runtime\n",
            "\t0.11s\t = Validation runtime\n",
            "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 112.53s of the 112.53s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
            "\t-19.2856\t = Validation score   (-root_mean_squared_error)\n",
            "\t3.44s\t = Training   runtime\n",
            "\t0.07s\t = Validation runtime\n",
            "Fitting model: XGBoost_BAG_L1 ... Training model for up to 107.74s of the 107.73s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.03%)\n",
            "\t-18.5863\t = Validation score   (-root_mean_squared_error)\n",
            "\t1.49s\t = Training   runtime\n",
            "\t0.05s\t = Validation runtime\n",
            "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 104.88s of the 104.87s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
            "\t-17.5922\t = Validation score   (-root_mean_squared_error)\n",
            "\t9.89s\t = Training   runtime\n",
            "\t1.04s\t = Validation runtime\n",
            "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 93.50s of the 93.49s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.07%)\n",
            "\t-18.7696\t = Validation score   (-root_mean_squared_error)\n",
            "\t3.67s\t = Training   runtime\n",
            "\t0.06s\t = Validation runtime\n",
            "Fitting model: CatBoost_r177_BAG_L1 ... Training model for up to 88.45s of the 88.45s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.03%)\n",
            "\t-18.2986\t = Validation score   (-root_mean_squared_error)\n",
            "\t2.85s\t = Training   runtime\n",
            "\t0.24s\t = Validation runtime\n",
            "Fitting model: NeuralNetTorch_r79_BAG_L1 ... Training model for up to 84.22s of the 84.22s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
            "\t-19.4724\t = Validation score   (-root_mean_squared_error)\n",
            "\t7.53s\t = Training   runtime\n",
            "\t1.09s\t = Validation runtime\n",
            "Fitting model: LightGBM_r131_BAG_L1 ... Training model for up to 75.19s of the 75.19s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.03%)\n",
            "\t-18.7921\t = Validation score   (-root_mean_squared_error)\n",
            "\t1.68s\t = Training   runtime\n",
            "\t0.03s\t = Validation runtime\n",
            "Fitting model: NeuralNetFastAI_r191_BAG_L1 ... Training model for up to 72.15s of the 72.14s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
            "\t-20.5723\t = Validation score   (-root_mean_squared_error)\n",
            "\t4.15s\t = Training   runtime\n",
            "\t0.1s\t = Validation runtime\n",
            "Fitting model: CatBoost_r9_BAG_L1 ... Training model for up to 66.60s of the 66.60s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.13%)\n",
            "\t-17.9865\t = Validation score   (-root_mean_squared_error)\n",
            "\t6.97s\t = Training   runtime\n",
            "\t0.25s\t = Validation runtime\n",
            "Fitting model: LightGBM_r96_BAG_L1 ... Training model for up to 58.27s of the 58.26s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.01%)\n",
            "\t-19.3045\t = Validation score   (-root_mean_squared_error)\n",
            "\t4.17s\t = Training   runtime\n",
            "\t0.06s\t = Validation runtime\n",
            "Fitting model: NeuralNetTorch_r22_BAG_L1 ... Training model for up to 52.74s of the 52.73s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
            "\t-17.2429\t = Validation score   (-root_mean_squared_error)\n",
            "\t14.1s\t = Training   runtime\n",
            "\t1.13s\t = Validation runtime\n",
            "Fitting model: XGBoost_r33_BAG_L1 ... Training model for up to 37.13s of the 37.12s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.26%)\n",
            "\t-19.022\t = Validation score   (-root_mean_squared_error)\n",
            "\t3.22s\t = Training   runtime\n",
            "\t0.05s\t = Validation runtime\n",
            "Fitting model: ExtraTrees_r42_BAG_L1 ... Training model for up to 32.57s of the 32.56s of remaining time.\n",
            "\t-19.1078\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.51s\t = Training   runtime\n",
            "\t0.09s\t = Validation runtime\n",
            "Fitting model: CatBoost_r137_BAG_L1 ... Training model for up to 31.82s of the 31.81s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.01%)\n",
            "\t-18.3648\t = Validation score   (-root_mean_squared_error)\n",
            "\t6.74s\t = Training   runtime\n",
            "\t0.26s\t = Validation runtime\n",
            "Fitting model: NeuralNetFastAI_r102_BAG_L1 ... Training model for up to 23.72s of the 23.72s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.00%)\n",
            "\t-21.9509\t = Validation score   (-root_mean_squared_error)\n",
            "\t5.89s\t = Training   runtime\n",
            "\t0.16s\t = Validation runtime\n",
            "Fitting model: CatBoost_r13_BAG_L1 ... Training model for up to 16.52s of the 16.51s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=24, gpus=0, memory=0.13%)\n",
            "\t-18.32\t = Validation score   (-root_mean_squared_error)\n",
            "\t13.17s\t = Training   runtime\n",
            "\t0.27s\t = Validation runtime\n",
            "Fitting model: RandomForest_r195_BAG_L1 ... Training model for up to 1.93s of the 1.92s of remaining time.\n",
            "\t-19.0544\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.76s\t = Training   runtime\n",
            "\t0.11s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ... Training model for up to 125.09s of the 0.87s of remaining time.\n",
            "\tEnsemble Weights: {'NeuralNetTorch_r22_BAG_L1': 0.6, 'LightGBMXT_BAG_L1': 0.2, 'CatBoost_r9_BAG_L1': 0.12, 'XGBoost_BAG_L1': 0.08}\n",
            "\t-16.8707\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 126.87s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 30.0 rows/s (44 batch size)\n",
            "Automatically performing refit_full as a post-fit operation (due to `.fit(..., refit_full=True)`\n",
            "Refitting models via `predictor.refit_full` using all of the data (combined train and validation)...\n",
            "\tModels trained in this way will have the suffix \"_FULL\" and have NaN validation score.\n",
            "\tThis process is not bound by time_limit, but should take less time than the original `predictor.fit` call.\n",
            "\tTo learn more, refer to the `.refit_full` method docstring which explains how \"_FULL\" models differ from normal models.\n",
            "Fitting model: KNeighborsUnif_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
            "\t0.03s\t = Training   runtime\n",
            "\t0.32s\t = Validation runtime\n",
            "Fitting model: KNeighborsDist_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
            "\t0.03s\t = Training   runtime\n",
            "\t0.18s\t = Validation runtime\n",
            "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
            "Fitting model: LightGBMXT_BAG_L1_FULL ...\n",
            "\t0.95s\t = Training   runtime\n",
            "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
            "Fitting model: LightGBM_BAG_L1_FULL ...\n",
            "\t0.66s\t = Training   runtime\n",
            "Fitting model: RandomForestMSE_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
            "\t0.62s\t = Training   runtime\n",
            "\t0.1s\t = Validation runtime\n",
            "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
            "Fitting model: CatBoost_BAG_L1_FULL ...\n",
            "\t2.46s\t = Training   runtime\n",
            "Fitting model: ExtraTreesMSE_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
            "\t0.58s\t = Training   runtime\n",
            "\t0.11s\t = Validation runtime\n",
            "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
            "Fitting model: NeuralNetFastAI_BAG_L1_FULL ...\n",
            "\tStopping at the best epoch learned earlier - 12.\n",
            "\t0.78s\t = Training   runtime\n",
            "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
            "Fitting model: XGBoost_BAG_L1_FULL ...\n",
            "\t0.23s\t = Training   runtime\n",
            "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
            "Fitting model: NeuralNetTorch_BAG_L1_FULL ...\n",
            "\t4.2s\t = Training   runtime\n",
            "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
            "Fitting model: LightGBMLarge_BAG_L1_FULL ...\n",
            "\t1.39s\t = Training   runtime\n",
            "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
            "Fitting model: CatBoost_r177_BAG_L1_FULL ...\n",
            "\t0.62s\t = Training   runtime\n",
            "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
            "Fitting model: NeuralNetTorch_r79_BAG_L1_FULL ...\n",
            "\t1.12s\t = Training   runtime\n",
            "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
            "Fitting model: LightGBM_r131_BAG_L1_FULL ...\n",
            "\t1.73s\t = Training   runtime\n",
            "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
            "Fitting model: NeuralNetFastAI_r191_BAG_L1_FULL ...\n",
            "\tStopping at the best epoch learned earlier - 14.\n",
            "\t0.8s\t = Training   runtime\n",
            "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
            "Fitting model: CatBoost_r9_BAG_L1_FULL ...\n",
            "\t1.74s\t = Training   runtime\n",
            "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
            "Fitting model: LightGBM_r96_BAG_L1_FULL ...\n",
            "\t4.92s\t = Training   runtime\n",
            "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
            "Fitting model: NeuralNetTorch_r22_BAG_L1_FULL ...\n",
            "\t5.91s\t = Training   runtime\n",
            "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
            "Fitting model: XGBoost_r33_BAG_L1_FULL ...\n",
            "\t1.38s\t = Training   runtime\n",
            "Fitting model: ExtraTrees_r42_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
            "\t0.51s\t = Training   runtime\n",
            "\t0.09s\t = Validation runtime\n",
            "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
            "Fitting model: CatBoost_r137_BAG_L1_FULL ...\n",
            "\t1.88s\t = Training   runtime\n",
            "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
            "Fitting model: NeuralNetFastAI_r102_BAG_L1_FULL ...\n",
            "\tStopping at the best epoch learned earlier - 7.\n",
            "\t1.03s\t = Training   runtime\n",
            "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
            "Fitting model: CatBoost_r13_BAG_L1_FULL ...\n",
            "\t9.31s\t = Training   runtime\n",
            "Fitting model: RandomForest_r195_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
            "\t0.76s\t = Training   runtime\n",
            "\t0.11s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2_FULL | Skipping fit via cloning parent ...\n",
            "\tEnsemble Weights: {'NeuralNetTorch_r22_BAG_L1': 0.6, 'LightGBMXT_BAG_L1': 0.2, 'CatBoost_r9_BAG_L1': 0.12, 'XGBoost_BAG_L1': 0.08}\n",
            "\t0.01s\t = Training   runtime\n",
            "Updated best model to \"WeightedEnsemble_L2_FULL\" (Previously \"WeightedEnsemble_L2\"). AutoGluon will default to using \"WeightedEnsemble_L2_FULL\" for predict() and predict_proba().\n",
            "Refit complete, total runtime = 45.29s ... Best model: \"WeightedEnsemble_L2_FULL\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/home/chris/courses/wt-25-ml-in-ms/wt_25_ml_in_ms/08_ex/AutogluonModels/ag-20250108_095935\")\n"
          ]
        }
      ],
      "source": [
        "predictor = TabularPredictor(\n",
        "    label=\"value\",\n",
        "    problem_type=\"regression\",\n",
        ").fit(train_data, time_limit=200, presets=\"high_quality\")\n",
        "\n",
        "# presets at https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'pred')"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import root_mean_squared_error, r2_score\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "y_pred = predictor.predict(test_data.drop(columns=[\"value\"]))\n",
        "\n",
        "\n",
        "rmse = root_mean_squared_error(test_data[\"value\"], y_pred)\n",
        "r2 = r2_score(test_data[\"value\"], y_pred)\n",
        "\n",
        "\n",
        "ax.plot(y_pred, test_data[\"value\"], \"o\")\n",
        "ax.plot([0, 100], [0, 100], \"k--\")\n",
        "ax.text(0.1, 0.9, f\"R2 = {r2:.3f}\", transform=ax.transAxes)\n",
        "ax.text(0.1, 0.85, f\"RMSE = {rmse:.3f}\", transform=ax.transAxes)\n",
        "ax.set_title(\"Testdata set\")\n",
        "ax.set_ylabel(\"truth\")\n",
        "ax.set_xlabel(\"pred\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>model</th>\n",
              "      <th>score_test</th>\n",
              "      <th>score_val</th>\n",
              "      <th>eval_metric</th>\n",
              "      <th>pred_time_test</th>\n",
              "      <th>pred_time_val</th>\n",
              "      <th>fit_time</th>\n",
              "      <th>pred_time_test_marginal</th>\n",
              "      <th>pred_time_val_marginal</th>\n",
              "      <th>fit_time_marginal</th>\n",
              "      <th>stack_level</th>\n",
              "      <th>can_infer</th>\n",
              "      <th>fit_order</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>WeightedEnsemble_L2_FULL</td>\n",
              "      <td>-18.067172</td>\n",
              "      <td>NaN</td>\n",
              "      <td>root_mean_squared_error</td>\n",
              "      <td>0.218737</td>\n",
              "      <td>NaN</td>\n",
              "      <td>8.837717</td>\n",
              "      <td>0.002305</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.009048</td>\n",
              "      <td>2</td>\n",
              "      <td>True</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>XGBoost_r33_BAG_L1_FULL</td>\n",
              "      <td>-18.543248</td>\n",
              "      <td>NaN</td>\n",
              "      <td>root_mean_squared_error</td>\n",
              "      <td>0.039796</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.377994</td>\n",
              "      <td>0.039796</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.377994</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "      <td>44</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>LightGBM_r131_BAG_L1_FULL</td>\n",
              "      <td>-18.559354</td>\n",
              "      <td>NaN</td>\n",
              "      <td>root_mean_squared_error</td>\n",
              "      <td>0.023207</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.733645</td>\n",
              "      <td>0.023207</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.733645</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "      <td>39</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>CatBoost_r9_BAG_L1_FULL</td>\n",
              "      <td>-18.586058</td>\n",
              "      <td>NaN</td>\n",
              "      <td>root_mean_squared_error</td>\n",
              "      <td>0.033301</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.737180</td>\n",
              "      <td>0.033301</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.737180</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "      <td>41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>LightGBM_BAG_L1_FULL</td>\n",
              "      <td>-18.646686</td>\n",
              "      <td>NaN</td>\n",
              "      <td>root_mean_squared_error</td>\n",
              "      <td>0.023527</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.655856</td>\n",
              "      <td>0.023527</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.655856</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "      <td>29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>CatBoost_BAG_L1_FULL</td>\n",
              "      <td>-18.655791</td>\n",
              "      <td>NaN</td>\n",
              "      <td>root_mean_squared_error</td>\n",
              "      <td>0.032476</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.463821</td>\n",
              "      <td>0.032476</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.463821</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "      <td>31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>NeuralNetTorch_r22_BAG_L1_FULL</td>\n",
              "      <td>-18.678836</td>\n",
              "      <td>NaN</td>\n",
              "      <td>root_mean_squared_error</td>\n",
              "      <td>0.123261</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5.909173</td>\n",
              "      <td>0.123261</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5.909173</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "      <td>43</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>ExtraTrees_r42_BAG_L1_FULL</td>\n",
              "      <td>-18.708754</td>\n",
              "      <td>NaN</td>\n",
              "      <td>root_mean_squared_error</td>\n",
              "      <td>0.077383</td>\n",
              "      <td>0.094190</td>\n",
              "      <td>0.513573</td>\n",
              "      <td>0.077383</td>\n",
              "      <td>0.094190</td>\n",
              "      <td>0.513573</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "      <td>45</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>ExtraTrees_r42_BAG_L1</td>\n",
              "      <td>-18.708754</td>\n",
              "      <td>-19.107821</td>\n",
              "      <td>root_mean_squared_error</td>\n",
              "      <td>0.080207</td>\n",
              "      <td>0.094190</td>\n",
              "      <td>0.513573</td>\n",
              "      <td>0.080207</td>\n",
              "      <td>0.094190</td>\n",
              "      <td>0.513573</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>CatBoost_r13_BAG_L1_FULL</td>\n",
              "      <td>-18.738863</td>\n",
              "      <td>NaN</td>\n",
              "      <td>root_mean_squared_error</td>\n",
              "      <td>0.030625</td>\n",
              "      <td>NaN</td>\n",
              "      <td>9.305657</td>\n",
              "      <td>0.030625</td>\n",
              "      <td>NaN</td>\n",
              "      <td>9.305657</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "      <td>48</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>ExtraTreesMSE_BAG_L1_FULL</td>\n",
              "      <td>-18.786852</td>\n",
              "      <td>NaN</td>\n",
              "      <td>root_mean_squared_error</td>\n",
              "      <td>0.076163</td>\n",
              "      <td>0.107043</td>\n",
              "      <td>0.581456</td>\n",
              "      <td>0.076163</td>\n",
              "      <td>0.107043</td>\n",
              "      <td>0.581456</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "      <td>32</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>ExtraTreesMSE_BAG_L1</td>\n",
              "      <td>-18.786852</td>\n",
              "      <td>-19.071912</td>\n",
              "      <td>root_mean_squared_error</td>\n",
              "      <td>0.076499</td>\n",
              "      <td>0.107043</td>\n",
              "      <td>0.581456</td>\n",
              "      <td>0.076499</td>\n",
              "      <td>0.107043</td>\n",
              "      <td>0.581456</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>RandomForest_r195_BAG_L1_FULL</td>\n",
              "      <td>-18.872749</td>\n",
              "      <td>NaN</td>\n",
              "      <td>root_mean_squared_error</td>\n",
              "      <td>0.068104</td>\n",
              "      <td>0.106630</td>\n",
              "      <td>0.764695</td>\n",
              "      <td>0.068104</td>\n",
              "      <td>0.106630</td>\n",
              "      <td>0.764695</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "      <td>49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>RandomForest_r195_BAG_L1</td>\n",
              "      <td>-18.872749</td>\n",
              "      <td>-19.054356</td>\n",
              "      <td>root_mean_squared_error</td>\n",
              "      <td>0.684924</td>\n",
              "      <td>0.106630</td>\n",
              "      <td>0.764695</td>\n",
              "      <td>0.684924</td>\n",
              "      <td>0.106630</td>\n",
              "      <td>0.764695</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "      <td>24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>NeuralNetFastAI_r102_BAG_L1_FULL</td>\n",
              "      <td>-18.927476</td>\n",
              "      <td>NaN</td>\n",
              "      <td>root_mean_squared_error</td>\n",
              "      <td>0.024694</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.031963</td>\n",
              "      <td>0.024694</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.031963</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "      <td>47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>RandomForestMSE_BAG_L1_FULL</td>\n",
              "      <td>-19.139208</td>\n",
              "      <td>NaN</td>\n",
              "      <td>root_mean_squared_error</td>\n",
              "      <td>0.075086</td>\n",
              "      <td>0.095405</td>\n",
              "      <td>0.616709</td>\n",
              "      <td>0.075086</td>\n",
              "      <td>0.095405</td>\n",
              "      <td>0.616709</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "      <td>30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>RandomForestMSE_BAG_L1</td>\n",
              "      <td>-19.139208</td>\n",
              "      <td>-19.003386</td>\n",
              "      <td>root_mean_squared_error</td>\n",
              "      <td>0.080355</td>\n",
              "      <td>0.095405</td>\n",
              "      <td>0.616709</td>\n",
              "      <td>0.080355</td>\n",
              "      <td>0.095405</td>\n",
              "      <td>0.616709</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>LightGBMXT_BAG_L1_FULL</td>\n",
              "      <td>-19.174944</td>\n",
              "      <td>NaN</td>\n",
              "      <td>root_mean_squared_error</td>\n",
              "      <td>0.022352</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.951109</td>\n",
              "      <td>0.022352</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.951109</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "      <td>28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>CatBoost_r137_BAG_L1_FULL</td>\n",
              "      <td>-19.409226</td>\n",
              "      <td>NaN</td>\n",
              "      <td>root_mean_squared_error</td>\n",
              "      <td>0.031710</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.882147</td>\n",
              "      <td>0.031710</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.882147</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "      <td>46</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>CatBoost_r177_BAG_L1_FULL</td>\n",
              "      <td>-19.496411</td>\n",
              "      <td>NaN</td>\n",
              "      <td>root_mean_squared_error</td>\n",
              "      <td>0.030262</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.618421</td>\n",
              "      <td>0.030262</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.618421</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "      <td>37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>KNeighborsDist_BAG_L1</td>\n",
              "      <td>-19.674458</td>\n",
              "      <td>-19.863874</td>\n",
              "      <td>root_mean_squared_error</td>\n",
              "      <td>0.111417</td>\n",
              "      <td>0.175396</td>\n",
              "      <td>0.033260</td>\n",
              "      <td>0.111417</td>\n",
              "      <td>0.175396</td>\n",
              "      <td>0.033260</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>KNeighborsDist_BAG_L1_FULL</td>\n",
              "      <td>-19.674458</td>\n",
              "      <td>NaN</td>\n",
              "      <td>root_mean_squared_error</td>\n",
              "      <td>0.154111</td>\n",
              "      <td>0.175396</td>\n",
              "      <td>0.033260</td>\n",
              "      <td>0.154111</td>\n",
              "      <td>0.175396</td>\n",
              "      <td>0.033260</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "      <td>27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>NeuralNetTorch_BAG_L1_FULL</td>\n",
              "      <td>-19.840232</td>\n",
              "      <td>NaN</td>\n",
              "      <td>root_mean_squared_error</td>\n",
              "      <td>0.127087</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.204906</td>\n",
              "      <td>0.127087</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.204906</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "      <td>35</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>KNeighborsUnif_BAG_L1</td>\n",
              "      <td>-19.928502</td>\n",
              "      <td>-20.269091</td>\n",
              "      <td>root_mean_squared_error</td>\n",
              "      <td>0.101998</td>\n",
              "      <td>0.317028</td>\n",
              "      <td>0.032219</td>\n",
              "      <td>0.101998</td>\n",
              "      <td>0.317028</td>\n",
              "      <td>0.032219</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>KNeighborsUnif_BAG_L1_FULL</td>\n",
              "      <td>-19.928502</td>\n",
              "      <td>NaN</td>\n",
              "      <td>root_mean_squared_error</td>\n",
              "      <td>0.118168</td>\n",
              "      <td>0.317028</td>\n",
              "      <td>0.032219</td>\n",
              "      <td>0.118168</td>\n",
              "      <td>0.317028</td>\n",
              "      <td>0.032219</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "      <td>26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>XGBoost_BAG_L1_FULL</td>\n",
              "      <td>-19.937957</td>\n",
              "      <td>NaN</td>\n",
              "      <td>root_mean_squared_error</td>\n",
              "      <td>0.037518</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.231208</td>\n",
              "      <td>0.037518</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.231208</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "      <td>34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>LightGBM_r96_BAG_L1_FULL</td>\n",
              "      <td>-20.106489</td>\n",
              "      <td>NaN</td>\n",
              "      <td>root_mean_squared_error</td>\n",
              "      <td>0.035789</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.918843</td>\n",
              "      <td>0.035789</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.918843</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "      <td>42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>NeuralNetFastAI_BAG_L1_FULL</td>\n",
              "      <td>-20.427177</td>\n",
              "      <td>NaN</td>\n",
              "      <td>root_mean_squared_error</td>\n",
              "      <td>0.024603</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.780399</td>\n",
              "      <td>0.024603</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.780399</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "      <td>33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>LightGBMLarge_BAG_L1_FULL</td>\n",
              "      <td>-21.375136</td>\n",
              "      <td>NaN</td>\n",
              "      <td>root_mean_squared_error</td>\n",
              "      <td>0.021000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.393509</td>\n",
              "      <td>0.021000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.393509</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "      <td>36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>NeuralNetTorch_r79_BAG_L1_FULL</td>\n",
              "      <td>-21.394803</td>\n",
              "      <td>NaN</td>\n",
              "      <td>root_mean_squared_error</td>\n",
              "      <td>0.124708</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.122296</td>\n",
              "      <td>0.124708</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.122296</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "      <td>38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>NeuralNetFastAI_r191_BAG_L1_FULL</td>\n",
              "      <td>-22.062117</td>\n",
              "      <td>NaN</td>\n",
              "      <td>root_mean_squared_error</td>\n",
              "      <td>0.018479</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.795835</td>\n",
              "      <td>0.018479</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.795835</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "      <td>40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>WeightedEnsemble_L2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-16.870732</td>\n",
              "      <td>root_mean_squared_error</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.466465</td>\n",
              "      <td>23.548830</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000283</td>\n",
              "      <td>0.009048</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "      <td>25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>NeuralNetTorch_r22_BAG_L1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-17.242930</td>\n",
              "      <td>root_mean_squared_error</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.133759</td>\n",
              "      <td>14.099153</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.133759</td>\n",
              "      <td>14.099153</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>NeuralNetTorch_BAG_L1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-17.592214</td>\n",
              "      <td>root_mean_squared_error</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.039125</td>\n",
              "      <td>9.887064</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.039125</td>\n",
              "      <td>9.887064</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>CatBoost_r9_BAG_L1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-17.986546</td>\n",
              "      <td>root_mean_squared_error</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.254180</td>\n",
              "      <td>6.967053</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.254180</td>\n",
              "      <td>6.967053</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>LightGBMXT_BAG_L1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-18.137322</td>\n",
              "      <td>root_mean_squared_error</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.027337</td>\n",
              "      <td>0.984477</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.027337</td>\n",
              "      <td>0.984477</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>CatBoost_r177_BAG_L1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-18.298619</td>\n",
              "      <td>root_mean_squared_error</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.243659</td>\n",
              "      <td>2.846375</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.243659</td>\n",
              "      <td>2.846375</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>CatBoost_BAG_L1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-18.314984</td>\n",
              "      <td>root_mean_squared_error</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.247737</td>\n",
              "      <td>4.045255</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.247737</td>\n",
              "      <td>4.045255</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>CatBoost_r13_BAG_L1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-18.319951</td>\n",
              "      <td>root_mean_squared_error</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.266747</td>\n",
              "      <td>13.169102</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.266747</td>\n",
              "      <td>13.169102</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>CatBoost_r137_BAG_L1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-18.364763</td>\n",
              "      <td>root_mean_squared_error</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.260198</td>\n",
              "      <td>6.736447</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.260198</td>\n",
              "      <td>6.736447</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>XGBoost_BAG_L1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-18.586261</td>\n",
              "      <td>root_mean_squared_error</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.050905</td>\n",
              "      <td>1.489098</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.050905</td>\n",
              "      <td>1.489098</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>LightGBM_BAG_L1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-18.628670</td>\n",
              "      <td>root_mean_squared_error</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.025469</td>\n",
              "      <td>0.948164</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.025469</td>\n",
              "      <td>0.948164</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>LightGBMLarge_BAG_L1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-18.769573</td>\n",
              "      <td>root_mean_squared_error</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.055189</td>\n",
              "      <td>3.670021</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.055189</td>\n",
              "      <td>3.670021</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>LightGBM_r131_BAG_L1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-18.792103</td>\n",
              "      <td>root_mean_squared_error</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.033377</td>\n",
              "      <td>1.683693</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.033377</td>\n",
              "      <td>1.683693</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>XGBoost_r33_BAG_L1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-19.021983</td>\n",
              "      <td>root_mean_squared_error</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.048336</td>\n",
              "      <td>3.219003</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.048336</td>\n",
              "      <td>3.219003</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>NeuralNetFastAI_BAG_L1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-19.285642</td>\n",
              "      <td>root_mean_squared_error</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.074873</td>\n",
              "      <td>3.440730</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.074873</td>\n",
              "      <td>3.440730</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>LightGBM_r96_BAG_L1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-19.304496</td>\n",
              "      <td>root_mean_squared_error</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.057870</td>\n",
              "      <td>4.166188</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.057870</td>\n",
              "      <td>4.166188</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>NeuralNetTorch_r79_BAG_L1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-19.472413</td>\n",
              "      <td>root_mean_squared_error</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.086087</td>\n",
              "      <td>7.531734</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.086087</td>\n",
              "      <td>7.531734</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>NeuralNetFastAI_r191_BAG_L1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-20.572322</td>\n",
              "      <td>root_mean_squared_error</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.099377</td>\n",
              "      <td>4.147061</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.099377</td>\n",
              "      <td>4.147061</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>NeuralNetFastAI_r102_BAG_L1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-21.950864</td>\n",
              "      <td>root_mean_squared_error</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.162241</td>\n",
              "      <td>5.887094</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.162241</td>\n",
              "      <td>5.887094</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>22</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                               model  score_test  score_val  \\\n",
              "0           WeightedEnsemble_L2_FULL  -18.067172        NaN   \n",
              "1            XGBoost_r33_BAG_L1_FULL  -18.543248        NaN   \n",
              "2          LightGBM_r131_BAG_L1_FULL  -18.559354        NaN   \n",
              "3            CatBoost_r9_BAG_L1_FULL  -18.586058        NaN   \n",
              "4               LightGBM_BAG_L1_FULL  -18.646686        NaN   \n",
              "5               CatBoost_BAG_L1_FULL  -18.655791        NaN   \n",
              "6     NeuralNetTorch_r22_BAG_L1_FULL  -18.678836        NaN   \n",
              "7         ExtraTrees_r42_BAG_L1_FULL  -18.708754        NaN   \n",
              "8              ExtraTrees_r42_BAG_L1  -18.708754 -19.107821   \n",
              "9           CatBoost_r13_BAG_L1_FULL  -18.738863        NaN   \n",
              "10         ExtraTreesMSE_BAG_L1_FULL  -18.786852        NaN   \n",
              "11              ExtraTreesMSE_BAG_L1  -18.786852 -19.071912   \n",
              "12     RandomForest_r195_BAG_L1_FULL  -18.872749        NaN   \n",
              "13          RandomForest_r195_BAG_L1  -18.872749 -19.054356   \n",
              "14  NeuralNetFastAI_r102_BAG_L1_FULL  -18.927476        NaN   \n",
              "15       RandomForestMSE_BAG_L1_FULL  -19.139208        NaN   \n",
              "16            RandomForestMSE_BAG_L1  -19.139208 -19.003386   \n",
              "17            LightGBMXT_BAG_L1_FULL  -19.174944        NaN   \n",
              "18         CatBoost_r137_BAG_L1_FULL  -19.409226        NaN   \n",
              "19         CatBoost_r177_BAG_L1_FULL  -19.496411        NaN   \n",
              "20             KNeighborsDist_BAG_L1  -19.674458 -19.863874   \n",
              "21        KNeighborsDist_BAG_L1_FULL  -19.674458        NaN   \n",
              "22        NeuralNetTorch_BAG_L1_FULL  -19.840232        NaN   \n",
              "23             KNeighborsUnif_BAG_L1  -19.928502 -20.269091   \n",
              "24        KNeighborsUnif_BAG_L1_FULL  -19.928502        NaN   \n",
              "25               XGBoost_BAG_L1_FULL  -19.937957        NaN   \n",
              "26          LightGBM_r96_BAG_L1_FULL  -20.106489        NaN   \n",
              "27       NeuralNetFastAI_BAG_L1_FULL  -20.427177        NaN   \n",
              "28         LightGBMLarge_BAG_L1_FULL  -21.375136        NaN   \n",
              "29    NeuralNetTorch_r79_BAG_L1_FULL  -21.394803        NaN   \n",
              "30  NeuralNetFastAI_r191_BAG_L1_FULL  -22.062117        NaN   \n",
              "31               WeightedEnsemble_L2         NaN -16.870732   \n",
              "32         NeuralNetTorch_r22_BAG_L1         NaN -17.242930   \n",
              "33             NeuralNetTorch_BAG_L1         NaN -17.592214   \n",
              "34                CatBoost_r9_BAG_L1         NaN -17.986546   \n",
              "35                 LightGBMXT_BAG_L1         NaN -18.137322   \n",
              "36              CatBoost_r177_BAG_L1         NaN -18.298619   \n",
              "37                   CatBoost_BAG_L1         NaN -18.314984   \n",
              "38               CatBoost_r13_BAG_L1         NaN -18.319951   \n",
              "39              CatBoost_r137_BAG_L1         NaN -18.364763   \n",
              "40                    XGBoost_BAG_L1         NaN -18.586261   \n",
              "41                   LightGBM_BAG_L1         NaN -18.628670   \n",
              "42              LightGBMLarge_BAG_L1         NaN -18.769573   \n",
              "43              LightGBM_r131_BAG_L1         NaN -18.792103   \n",
              "44                XGBoost_r33_BAG_L1         NaN -19.021983   \n",
              "45            NeuralNetFastAI_BAG_L1         NaN -19.285642   \n",
              "46               LightGBM_r96_BAG_L1         NaN -19.304496   \n",
              "47         NeuralNetTorch_r79_BAG_L1         NaN -19.472413   \n",
              "48       NeuralNetFastAI_r191_BAG_L1         NaN -20.572322   \n",
              "49       NeuralNetFastAI_r102_BAG_L1         NaN -21.950864   \n",
              "\n",
              "                eval_metric  pred_time_test  pred_time_val   fit_time  \\\n",
              "0   root_mean_squared_error        0.218737            NaN   8.837717   \n",
              "1   root_mean_squared_error        0.039796            NaN   1.377994   \n",
              "2   root_mean_squared_error        0.023207            NaN   1.733645   \n",
              "3   root_mean_squared_error        0.033301            NaN   1.737180   \n",
              "4   root_mean_squared_error        0.023527            NaN   0.655856   \n",
              "5   root_mean_squared_error        0.032476            NaN   2.463821   \n",
              "6   root_mean_squared_error        0.123261            NaN   5.909173   \n",
              "7   root_mean_squared_error        0.077383       0.094190   0.513573   \n",
              "8   root_mean_squared_error        0.080207       0.094190   0.513573   \n",
              "9   root_mean_squared_error        0.030625            NaN   9.305657   \n",
              "10  root_mean_squared_error        0.076163       0.107043   0.581456   \n",
              "11  root_mean_squared_error        0.076499       0.107043   0.581456   \n",
              "12  root_mean_squared_error        0.068104       0.106630   0.764695   \n",
              "13  root_mean_squared_error        0.684924       0.106630   0.764695   \n",
              "14  root_mean_squared_error        0.024694            NaN   1.031963   \n",
              "15  root_mean_squared_error        0.075086       0.095405   0.616709   \n",
              "16  root_mean_squared_error        0.080355       0.095405   0.616709   \n",
              "17  root_mean_squared_error        0.022352            NaN   0.951109   \n",
              "18  root_mean_squared_error        0.031710            NaN   1.882147   \n",
              "19  root_mean_squared_error        0.030262            NaN   0.618421   \n",
              "20  root_mean_squared_error        0.111417       0.175396   0.033260   \n",
              "21  root_mean_squared_error        0.154111       0.175396   0.033260   \n",
              "22  root_mean_squared_error        0.127087            NaN   4.204906   \n",
              "23  root_mean_squared_error        0.101998       0.317028   0.032219   \n",
              "24  root_mean_squared_error        0.118168       0.317028   0.032219   \n",
              "25  root_mean_squared_error        0.037518            NaN   0.231208   \n",
              "26  root_mean_squared_error        0.035789            NaN   4.918843   \n",
              "27  root_mean_squared_error        0.024603            NaN   0.780399   \n",
              "28  root_mean_squared_error        0.021000            NaN   1.393509   \n",
              "29  root_mean_squared_error        0.124708            NaN   1.122296   \n",
              "30  root_mean_squared_error        0.018479            NaN   0.795835   \n",
              "31  root_mean_squared_error             NaN       1.466465  23.548830   \n",
              "32  root_mean_squared_error             NaN       1.133759  14.099153   \n",
              "33  root_mean_squared_error             NaN       1.039125   9.887064   \n",
              "34  root_mean_squared_error             NaN       0.254180   6.967053   \n",
              "35  root_mean_squared_error             NaN       0.027337   0.984477   \n",
              "36  root_mean_squared_error             NaN       0.243659   2.846375   \n",
              "37  root_mean_squared_error             NaN       0.247737   4.045255   \n",
              "38  root_mean_squared_error             NaN       0.266747  13.169102   \n",
              "39  root_mean_squared_error             NaN       0.260198   6.736447   \n",
              "40  root_mean_squared_error             NaN       0.050905   1.489098   \n",
              "41  root_mean_squared_error             NaN       0.025469   0.948164   \n",
              "42  root_mean_squared_error             NaN       0.055189   3.670021   \n",
              "43  root_mean_squared_error             NaN       0.033377   1.683693   \n",
              "44  root_mean_squared_error             NaN       0.048336   3.219003   \n",
              "45  root_mean_squared_error             NaN       0.074873   3.440730   \n",
              "46  root_mean_squared_error             NaN       0.057870   4.166188   \n",
              "47  root_mean_squared_error             NaN       1.086087   7.531734   \n",
              "48  root_mean_squared_error             NaN       0.099377   4.147061   \n",
              "49  root_mean_squared_error             NaN       0.162241   5.887094   \n",
              "\n",
              "    pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  \\\n",
              "0                  0.002305                     NaN           0.009048   \n",
              "1                  0.039796                     NaN           1.377994   \n",
              "2                  0.023207                     NaN           1.733645   \n",
              "3                  0.033301                     NaN           1.737180   \n",
              "4                  0.023527                     NaN           0.655856   \n",
              "5                  0.032476                     NaN           2.463821   \n",
              "6                  0.123261                     NaN           5.909173   \n",
              "7                  0.077383                0.094190           0.513573   \n",
              "8                  0.080207                0.094190           0.513573   \n",
              "9                  0.030625                     NaN           9.305657   \n",
              "10                 0.076163                0.107043           0.581456   \n",
              "11                 0.076499                0.107043           0.581456   \n",
              "12                 0.068104                0.106630           0.764695   \n",
              "13                 0.684924                0.106630           0.764695   \n",
              "14                 0.024694                     NaN           1.031963   \n",
              "15                 0.075086                0.095405           0.616709   \n",
              "16                 0.080355                0.095405           0.616709   \n",
              "17                 0.022352                     NaN           0.951109   \n",
              "18                 0.031710                     NaN           1.882147   \n",
              "19                 0.030262                     NaN           0.618421   \n",
              "20                 0.111417                0.175396           0.033260   \n",
              "21                 0.154111                0.175396           0.033260   \n",
              "22                 0.127087                     NaN           4.204906   \n",
              "23                 0.101998                0.317028           0.032219   \n",
              "24                 0.118168                0.317028           0.032219   \n",
              "25                 0.037518                     NaN           0.231208   \n",
              "26                 0.035789                     NaN           4.918843   \n",
              "27                 0.024603                     NaN           0.780399   \n",
              "28                 0.021000                     NaN           1.393509   \n",
              "29                 0.124708                     NaN           1.122296   \n",
              "30                 0.018479                     NaN           0.795835   \n",
              "31                      NaN                0.000283           0.009048   \n",
              "32                      NaN                1.133759          14.099153   \n",
              "33                      NaN                1.039125           9.887064   \n",
              "34                      NaN                0.254180           6.967053   \n",
              "35                      NaN                0.027337           0.984477   \n",
              "36                      NaN                0.243659           2.846375   \n",
              "37                      NaN                0.247737           4.045255   \n",
              "38                      NaN                0.266747          13.169102   \n",
              "39                      NaN                0.260198           6.736447   \n",
              "40                      NaN                0.050905           1.489098   \n",
              "41                      NaN                0.025469           0.948164   \n",
              "42                      NaN                0.055189           3.670021   \n",
              "43                      NaN                0.033377           1.683693   \n",
              "44                      NaN                0.048336           3.219003   \n",
              "45                      NaN                0.074873           3.440730   \n",
              "46                      NaN                0.057870           4.166188   \n",
              "47                      NaN                1.086087           7.531734   \n",
              "48                      NaN                0.099377           4.147061   \n",
              "49                      NaN                0.162241           5.887094   \n",
              "\n",
              "    stack_level  can_infer  fit_order  \n",
              "0             2       True         50  \n",
              "1             1       True         44  \n",
              "2             1       True         39  \n",
              "3             1       True         41  \n",
              "4             1       True         29  \n",
              "5             1       True         31  \n",
              "6             1       True         43  \n",
              "7             1       True         45  \n",
              "8             1       True         20  \n",
              "9             1       True         48  \n",
              "10            1       True         32  \n",
              "11            1       True          7  \n",
              "12            1       True         49  \n",
              "13            1       True         24  \n",
              "14            1       True         47  \n",
              "15            1       True         30  \n",
              "16            1       True          5  \n",
              "17            1       True         28  \n",
              "18            1       True         46  \n",
              "19            1       True         37  \n",
              "20            1       True          2  \n",
              "21            1       True         27  \n",
              "22            1       True         35  \n",
              "23            1       True          1  \n",
              "24            1       True         26  \n",
              "25            1       True         34  \n",
              "26            1       True         42  \n",
              "27            1       True         33  \n",
              "28            1       True         36  \n",
              "29            1       True         38  \n",
              "30            1       True         40  \n",
              "31            2      False         25  \n",
              "32            1      False         18  \n",
              "33            1      False         10  \n",
              "34            1      False         16  \n",
              "35            1      False          3  \n",
              "36            1      False         12  \n",
              "37            1      False          6  \n",
              "38            1      False         23  \n",
              "39            1      False         21  \n",
              "40            1      False          9  \n",
              "41            1      False          4  \n",
              "42            1      False         11  \n",
              "43            1      False         14  \n",
              "44            1      False         19  \n",
              "45            1      False          8  \n",
              "46            1      False         17  \n",
              "47            1      False         13  \n",
              "48            1      False         15  \n",
              "49            1      False         22  "
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predictor.leaderboard(test_data)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}